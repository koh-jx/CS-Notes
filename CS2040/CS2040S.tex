\documentclass{article}

% Formatting purposes
\usepackage[margin=1in]{geometry}

% Hyperlink functionality
% For \url
\usepackage{hyperref}

% Graphics Functionality
% For \includegraphics, \graphicspath{} and \begin{figure}
\usepackage{graphicx, caption, subcaption}

% Table functionality
\usepackage{multirow, multicol}
\usepackage{attrib}

\usepackage{bm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}% http://ctan.org/pkg/booktabs
\newcommand{\tabitem}{~~\llap{\textbullet}~~}

\title{CS2040 Notes}
\author{Koh Jia Xian}

\begin{document}
    \maketitle
    \tableofcontents

    \pagebreak

    \section{Definitions}

    \subsection{Time and Space Complexity}

    \begin{itemize}
        \item Space Complexity = \textbf{Total} space ever allocated
        \item Amortized cost $T(n)$ if $\forall k \in \mathbb{Z}$, cost of operation is $\leqq kT(n)$
    \end{itemize}

    \subsubsection{Big O}

    $T(n) = O(f(n))$ if:
    \begin{enumerate}
        \item There exists a constant $c > 0$
        \item and a constant $n_{0} > 0$
    \end{enumerate}
    such that for all $n > n_{0}$, \\\\$\bm{T(n) \leq c f(n)}$\\\\
    \emph{ie) An upper bound above a certain size n; Always try to get the tightest bound}

    \subsubsection{Big Omega}

    $T(n) = \Omega(f(n))$ if:
    \begin{enumerate}
        \item There exists a constant $c > 0$
        \item and a constant $n_{0} > 0$
    \end{enumerate}
    such that for all $n > n_{0}$, \\\\$\bm{T(n) \geq c f(n)}$\\\\
    \emph{ie) A lower bound above a certain size n}

    \subsection{Pre and Post-conditions}

    \begin{table}[htbp]
        \begin{tabular}{ll}
            \textbf{Precondition} & Fact that is true when the function begins\\
            \textbf{Postcondition} & Fact that is true when the function ends\\
        \end{tabular}
    \end{table}

    \subsection{Invariants}
        \begin{tabular}{ll}
            \textbf{Invariants} & Relationship between variables that is \textbf{always true}.\\
            \textbf{Loop Invariants} & Relationship between variables that is true at the beginning (or end) of each iteration of a loop.\\
        \end{tabular}


    \subsection{Stability and In-Place sorting}
    When 2 of the same keys are sorted:
    \begin{itemize}
        \item If its value becomes out of order, \textbf{Unstable}
        \item \textbf{Stability: Preserving order of repeated elements}
    \end{itemize}

    General Rule-of-Thumb, if got swap here-swap there (ie \textbf{NOT IN-PLACE}), it is unstable

    \subsection{Probability and Expected Value}

    
    \begin{itemize}
        \item $E[X] = e_{1}p_{1} + e_{2}p_{2} + ... + e_{k}p_{k}$
        \item $E(A+B) = E(A) + E(B)$
    \end{itemize}


    \subsection{Trees and Graphs}

    \begin{tabular}{ll}
        \textbf{Successor} & Next largest value in the tree.\\
        \textbf{Height} & Number of edges on longest path from root to leaf.\\
        & \tabitem $h(v) = 0$ if v is a leaf\\
        & \tabitem $h(v) = max(h(v.left), h(v.right)) + 1$\\\\
        \textbf{Cut} of a graph is a partition of vertices into 2 disjoint subsets\\
        An edge crosses a cut if it has one vertex in each of the 2 sets\\
    \end{tabular}

    

    \pagebreak

    \section{Common Time Complexities}


    \begin{tabular}{|l|c|l|}
        \toprule
        \textbf{Recurrence} & \textbf{Complexity} & \textbf{Remarks}\\
        \toprule
        $T(n) = 2T(n/2) + O(n)$ & $O(nlogn)$ & Height of logn, n each 'level'\\
        \midrule
        $T(n) = T(n / 2) + O(1)$ & $O(logn)$ & Height of logn, 1 each 'level'\\
        \midrule
        $T(n) = 2T(n / 2) + O(1)$ & $O(n)$ & 1, 2, 4, ... n: Sum of GP\\
        \midrule
        $T(n) = T(n / 2) + O(n)$ & $O(n)$ & n, n/2, n/4 ... 1: Sum of GP\\ 
        \bottomrule
    \end{tabular}

    \subsection{AP GP Sums}

    \begin{tabular}{l}
        \toprule
        For AP, $S_{n} = \frac{1}{2}n(a_{1} + a_{n})$\\
        \tabitem If AP is 1, 2,...n, $S_{n} = \frac{n^{2} + n}{2} = O(n^{2})$\\
        \midrule
        For GP, $S_{n} = \frac{a(r^{n}-1)}{r-1} = \frac{a(1 - r^{n})}{1- r}$\\
        \tabitem Sum to $\infty$ $S_{\infty} = \frac{a}{1-r}$\\
        \tabitem If GP is 1, 2, 4...n, where a = n, $r = 1/2$, $S_{n} = \frac{a}{1-r} = \frac{n}{1-0.5} = O(n)$\\
    \end{tabular}

    \pagebreak

    \section{Binary Search}
    For a \textbf{sorted }array, take middle, compare to key: search LHS or RHS of mid.

    \begin{verbatim}
    int search(A, key, n)
        begin = 0
        end = n-1
        while begin < end do:
            mid = begin + (end-begin)/2;
            if key <= A[mid] then
                end = mid
            else begin = mid+1
        return (A[begin]==key) ? begin : -1    
    \end{verbatim}


    \begin{tabular}{ll}
        \toprule
        \textbf{Functionality} & \tabitem If element not in array, return index\\
        & \tabitem If element not in array, return -1\\
        \midrule
        \textbf{Precondition} & \tabitem Array is of size n\\
        & \tabitem Array is sorted\\
        \midrule
        \textbf{Postcondition} & If element is in the array: A[begin] = key\\
        \midrule
        \textbf{Invariant (Correctness)} & $A[begin] \leq key \leq A[end]$ \\
        & \tabitem \emph{The key is in the range of the Array}\\
        \midrule
        \textbf{Invariant (Speed)} & $(end-begin) \leq n/2^{k}$ in iteration k \\
        \bottomrule
    \end{tabular}

    \bigskip
    \textbf{Not just for searching Arrays:}
    \begin{enumerate}
        \item \begin{itemize}
            \item Assuming a complicated function,
            \item Assume function is always increasing: $complicatedFunction(i) < complicatedFunction(i+1)$
            \item $\therefore$ Find minimum value j such that $complicatedFunction(j) > 100$
        \end{itemize}

        \item Peak Finding (1 or 2 Dimensions)
        \item QuickSelect
    \end{enumerate}

    \pagebreak

    \subsection{Peak Finding}

    Want to find an index i such that $\bm{arr[i] \geq arr[i-1]}$ \&  $\bm{arr[i] \leq arr[i+1]}$

    \begin{verbatim}
    FindPeak(A, n)
        //Recurse on right
        if A[n/2+1] > A[n/2] then
            FindPeak(A[n/2+1..n], n/2)

        //Recurse on left
        else if A[n/2–1] > A[n/2] then
            FindPeak(A[1..n/2‐1], n/2)

        else A[n/2] is a peak; return n/2

    \end{verbatim}
    
    \begin{tabular}{ll}
        \toprule
        \textbf{Functionality} & On an unsorted array, find A peak: \\
        & local minimum or maximum (not a specific key)\\
        \midrule
        \textbf{Invariants (Correctness)} & \tabitem There exists a peak in the range $[begin, end]$\\
        & Every peak in $[begin, end]$ is a peak in $[1, n]$.\\
        \midrule
        \textbf{Running Time} & $T(n) = T(n/2) + \theta(1)$\\
        & Recurse for $log2(n)$ times\\
        & $\therefore O(logn)$\\
        \bottomrule
    \end{tabular}

    \subsection{Steep Peaks}

    Want to find a peak such that its left and right side are \textbf{strictly lower than it}.\\\\

    \begin{tabular}{ll}
        \toprule
        \textbf{Functionality} & On an unsorted array, find A peak: local minimum or maximum (not a specific key)\\
        & \textbf{If both sides are the same as mid, recurse both sides}\\
        \midrule
        \textbf{Running Time} & $T(n) = $\textbf{2}$T(n/2) + \theta(1)$\\
        & $ = 16T(n/16) + 8  + 4 + 2 + 1$\\
        & $...$\\
        & $ = nT(1) + n/2 + n/4 + ... + 1$ \\
        & $\bm{ = O(n)}$ \textbf{Sum of Geometric Progression}\\
        \bottomrule
    \end{tabular}

    \pagebreak

    \subsection{QuickSelect}

    Find kth smallest element\\

    \noindent Makes use of QuickSort's \hyperref[partition]{partition} to ensure that the kth smallest element 
    is before or after the randomly selected pivot

    \begin{verbatim}
    Select(A[1..n], n, k)
        if (n == 1) then return A[1];
        else Choose random pivot index pIndex.
            p = partition(A[1..n], n, pIndex)
            if (k == p) then return A[p];
            else if (k < p) then
                return Select(A[1..p–1], k)
            else if (k > p) then
                return Select(A[p+1], k – p)
    \end{verbatim}

    \noindent Recurrence: $T(n) = T(n/2) + O(n)$\\

    \noindent Time Complexity: $\bm{O(n)}$ (Sum of G.P.)

    \subsubsection{Paranoid Select}

    Repeatedly partition until at least n/10 in each half of partition

    $E[T(n)] \leq E[T(9n/10)] + E[num of partitions](n)$

    $\leq E[T(9n/10)] + 2n$

    $\leq O(n)$

    \pagebreak

    \section{Sorting}

    \subsection{Bubble Sort}

    Iteratively swap largest values to the top.

    \begin{verbatim}
    BubbleSort(A, n)
        repeat (until no swaps) :
            for j <- 1 to n-1
                if A[j] > A[j+1] then swap(A[j], A[j+1])

    \end{verbatim}
    
    \begin{tabular}{ll}
        \toprule
        \textbf{Loop Invariant} & At the end of iteration j, the biggest j items are correctly sorted \\
        & in the \textbf{final j positions} of the array.\\
        \midrule
        \textbf{Invariant (Correctnness)} & Sorted after n iterations\\
        \midrule
        \textbf{Running Time} & \\
        \tabitem Best Case & $O(n)$ [Already Sorted]\\
        \tabitem Average Case & $O(n^{2})$\\
        \tabitem Worst Case & $O(n^{2})$ [n iterations]\\
        \midrule
        \textbf{Space Consumption} & $O(1)$\\
        \midrule
        \textbf{Stability} & \textbf{Stable}, only swap elements that are different\\
        \bottomrule
    \end{tabular}

    \bigskip

    \subsection{Selection Sort}

    Find minimum element and swap it directly with the front.

    \begin{verbatim}
    SelectionSort(A, n)
        for j <- 1 to n-1:
            find minimum element A[j] in A[j..n]
            swap(A[j], A[k])
    \end{verbatim}
    
    \begin{tabular}{ll}
        \toprule
        \textbf{Loop Invariant} & At the end of iteration j: the smallest j items are correctly sorted \\
        & in the \textbf{first j positions} of the array.\\
        \midrule
        \textbf{Running Time} & $n + (n-1) + (n-2) + ... + 1$\\
        & $ = \frac{n(n-1)}{2}$ (Sum of A.P.)\\
        & $ = O(n^{2})$\\
        \tabitem Best Case & $O(n^{2})$ [If already Sorted, will swap anyway]\\
        \tabitem Average Case & $O(n^{2})$\\
        \tabitem Worst Case & $O(n^{2})$ [n swaps]\\
        \midrule
        \textbf{Space Consumption} & $O(1)$\\
        \midrule
        \textbf{Stability} & \textbf{Unstable}, swap changes order\\
        \bottomrule
    \end{tabular}

    \pagebreak

    \subsection{Insertion Sort}

    Iteratively swaps the current element into its rightful place in the sorted left side of the array.

    \begin{verbatim}
    InsertionSort(A, n)
        for j <- 2 to n
            key <- A[j]
            i <- j-1
            while (i > 0) and (A[i] >key)
                A[i+1] <- A[i]
                i <- i-1
            A[i+1] <- key
    \end{verbatim}
    
    \begin{tabular}{ll}
        \toprule
        \textbf{Loop Invariant} & At the end of iteration j: the \textbf{first j items} in the array \\
        &  are in sorted order.\\
        \midrule
        \textbf{Running Time} & $1 + 2 + 3 + ... + n$\\
        & $ = \frac{n(n-1)}{2}$ (Sum of A.P.)\\
        & $ = O(n^{2})$\\
        \tabitem Best Case & $O(n)$ [Already Sorted]\\
        \tabitem Average Case & $O(n^{2})$\\
        \tabitem Worst Case & $O(n^{2})$ [Inverse Sorted]\\
        \midrule
        \textbf{Space Consumption} & $O(1)$\\
        \midrule
        \textbf{Stability} & \textbf{Stable}, swap doesn't change order, \\
        & as long as implemented properly (\textbf{A[i] $>$ key})\\
        \bottomrule
    \end{tabular}

    \bigskip

    Insertion Sort can be fast(er than MergeSort!) if \textbf{List is mostly sorted}


    \subsection{MergeSort}

    \textbf{Divide-and-Conquer}, sort two halves, merge two sorted halves

    \begin{verbatim}
    MergeSort(A, n)
        if (n=1) then return;                                               //O(1)
        else:           
            X <- MergeSort(A[1..n/2], n/2);                                 //T(n/2)
            Y <- MergeSort(A[n/2+1, n], n/2);                               //T(n/2)
            return Merge (X,Y, n/2); //2 sorted halves combined together    //O(n)
    \end{verbatim}
    
    \begin{tabular}{ll}
        \toprule
        \textbf{Running Time} & \\
        Running Time of Merge & Given A and B of sizes n/2, $\bm{O(n)}$ to move each element back into list\\
        & $\therefore T(n) = O(1)$ (if $n = 1$)\\
        & $= 2T(n/2) + cn$ (if $n > 1$)\\
        & $\therefore$ Height of recursion tree $h = logn$, every level $cn$ operations\\
        & $\therefore T(n) = cnlogn$, $\bm{O(n) = nlogn}$\\
        \tabitem Best Case & $O(nlogn)$\\
        \tabitem Average Case & $O(nlogn)$\\
        \tabitem Worst Case & $O(nlogn)$\\
        \midrule
        \textbf{Space Consumption} & $\bm{O(n)}$ [Using 1 temporary array, Switch the order of A and B at
        every recursive call.]\\
        \midrule
        \textbf{Stability} & \textbf{Stable}\\
        \bottomrule
    \end{tabular}

    \bigskip

    MergeSort can be slower for \textbf{Smaller number of items to sort}

    \pagebreak

    \subsection{QuickSort}

    Separate larger and smaller than a chosen \textbf{pivot} (Partitioning), recursively sort both sub-arrays.
    
    \label{partition}
    
    \begin{verbatim}
     QuickSort(A[1..n], n)
        if (n==1) then return;
        else
            Choose pivot index pIndex   //How?
            p = partition(A[1..n], n, pIndex)
            x = QuickSort(A[1..p-1], p-1)
            y = QuickSort(A[p+1..n], n-p)

    //Returns the index of the pivot
    partition(A[1..n], n, pIndex)       // Assume no duplicates, n>1
        pivot = A[pIndex];              // pIndex is the index of pivot
        swap(A[1], A[pIndex]);          // store pivot in A[1]
        low = 2;                        // start after pivot in A[1]
        high = n+1;                     // Define: A[n+1] = Infinity
        while (low < high)
            while (A[low] < pivot) and (low < high) do low++;
            while (A[high] > pivot) and (low < high) do high– – ;
            if (low < high) then swap(A[low], A[high]);
        swap(A[1], A[low–1]);
        return low–1;
    \end{verbatim}
    
    \begin{tabular}{ll}
        \toprule
        \textbf{Invariants} & \tabitem For every $i \geq high$ : $A[i] > pivot$\\
        & \tabitem For every $1 < j < low$ : $A[j] < pivot$\\
        \textbf{Running Time} & \\
        Running Time of Partition & $O(n)$\\
        \tabitem Best Case & $O(nlogn)$\\
        \tabitem Average Case & $O(nlogn)$\\
        \tabitem Worst Case & $O(n^{2})$ [eg All elements duplicates]\\
        \midrule
        \textbf{Space Consumption} & $O(1)$\\
        & \emph{Extra Memory allows QuickSort to be stable}\\
        \midrule
        \textbf{Stability} & \textbf{Unstable}\\
        \bottomrule
    \end{tabular}


    

    \subsection{QuickSort Optimisations}

    \subsubsection{Base Case?}
    \begin{itemize}
        \item Unoptimized: Recurse to single-element arrays
        \item Switch to Insertion Sort for small arrays (Relies on fact that InsertionSort is fast for small arrays)
        \item Halt Recursion early, leaving small arrays unsorted. Then perform InsertionSort on entire array
    \end{itemize}
    
    \pagebreak

    \subsubsection{3-Way Partitioning}

    Deal with duplicates in arrays

    \begin{tabular}{ll}
        \textbf{Option 1} & \textbf{2-pass Partitioning}\\
        & 1. Regular Partition\\
        & 2. Pack Duplicates (of pivot) together\\
        \textbf{Option 2} & \textbf{1-pass Partitioning}\\
        & \tabitem Standard Solution\\
        & \tabitem Mantain Four Regions of Array (See Fig \ref{1pass})\\
    \end{tabular}

    \begin{figure}[htbp]
        \begin{center}
        \includegraphics[width=4in,keepaspectratio]{3-way.JPG}
        \caption{1-pass Partitioning}
        \label{1pass}
        \end{center}
    \end{figure}

    \begin{tabular}{ll}
        \textbf{ If }$bm{A[current] < pivot}$ & low++\\
        & Swap $A[current]$, $A[low]$\\
        & current++\\
        \textbf{ If} $bm{A[current] == pivot}$ & current++\\
        \textbf{ If} $bm{A[current] > pivot}$ & Swap $A[current]$, $A[high]$ \\
        & high--\\
    \end{tabular}

    \subsubsection{Choice of Pivot}
    In the worst case(s),
    
    \noindent\begin{tabular}{ll}
        \textbf{First Element} & A[1]\\
        \textbf{Last Element} & A[n]\\
        \textbf{Middle Element} & A[n/2]\\
        \textbf{Median of first, last and middle} & Median of the above 3\\
    \end{tabular}
    
    \noindent are equally bad, if \textbf{n executions of partition, sorting 1 element each: }\\
    
    $T(n) = T(n-1) + T(1) + n$
    
    (From Quicksort of n-1 elements + QuickSort on 1 element + Cost of partition on n elements)

    $\bm{\therefore O(n^{2})}$ \textbf{time}.\\

    \noindent\textbf{If can choose Median: Good Performance }$\bm{O(nlogn)}$\\
    
    \noindent\textbf{If could split array (1:10) : (9:10): Good Performance }$\bm{O(nlogn)}$\\\\
    $\therefore$ A pivot is \textbf{good} if divides array into 2 pieces, each of which is size \textbf{at least} $\bm{n/10}$

    \noindent\textbf{Choose pivot at random: PARANOID QUICKSORT}

    Repeat partition until $p > (1/10)n$ and $p < (9/10)n$, 

    Expected number of times to choose a good pivot: $10/8 \approx 2$
    
    $T(n) = T(n-1) + T(1) + 2n$\emph{(Expected no. of iterations to repeat is 2)} 
    
    Hence, worst-case expected time $\bm{= O(nlogn)}$


    \pagebreak

    \section{Sorting Summary}

    \begin{tabular}{|l||l|l|l|l|l|}
        \toprule
        \textbf{Name} & \textbf{Best Case} & \textbf{Average Case} & \textbf{Worst Case} & \textbf{Extra Memory} & \textbf{Stable}\\
        \midrule
        \midrule
        \textbf{Bubble Sort} & $O(n)$ & $O(n^{2})$ & $O(n^{2})$ & $O(1)$ & Yes\\
        % \hline
        \textbf{SelectionSort} & $O(n^{2})$ & $O(n^{2})$ & $O(n^{2})$ & $O(1)$ & No\\
        % \hline
        \textbf{Insertion Sort} & $O(n)$ & $O(n^{2})$ & $O(n^{2})$ & $O(1)$ & Yes\\
        % \hline
        \textbf{Merge Sort} & $O(nlogn)$ & $O(nlogn)$ & $O(nlogn)$ & $O(n)$ & Yes\\
        % \hline
        \textbf{Quick Sort} & $O(nlogn)$ & $O(nlogn)$ & $O(n^{2})$ & $O(1)$ & No\\
        \bottomrule
    \end{tabular}

    \subsection{Remarks}
    \begin{itemize}
        \item BubbleSort vs InsertionSort: InsertionSort faster for almost-sorted arrays
        \item Paranoid Quicksort Worstcase: $O(nlogn)$
        \item \textbf{Any others?}
    \end{itemize}

    \subsection{Invariants}
    \begin{tabular}{|l||l|}
        \toprule
        \textbf{Name} & \textbf{Invariant}\\
        \midrule
        \midrule
        \textbf{Bubble Sort} &  At the end of iteration j, the biggest j items are correctly sorted \\
        & in the \textbf{final j positions} of the array.\\
        \hline
        \textbf{SelectionSort} & At the end of iteration j: the smallest j items are correctly sorted \\
        & in the \textbf{first j positions} of the array.\\
        \hline
        \textbf{Insertion Sort} & At the end of iteration j: the \textbf{first j items} in the array \\
        &  are in sorted order.\\
        \hline
        \textbf{Merge Sort} & idk lmfao probably something about at the end of iteration j of merge\\
        & every $2^{j}$ group of items are in sorted order, where $2^{j} < n$ (???)\\
        & just pulling something out of my ass :)\\ 
        \hline
        \textbf{Quick Sort} & \tabitem For every $i \geq high$ : $A[i] > pivot$\\
        & \tabitem For every $1 < j < low$ : $A[j] < pivot$\\
        \bottomrule
    \end{tabular}


    \begin{tabular}{|l|c|l|}
        \toprule
        \textbf{Recurrence} & \textbf{Complexity} & \textbf{Remarks}\\
        \toprule
        $T(n) = 2T(n/2) + O(n)$ & $O(nlogn)$ & Height of logn, n each 'level'\\
        \midrule
        $T(n) = T(n / 2) + O(1)$ & $O(logn)$ & Height of logn, 1 each 'level'\\
        \midrule
        $T(n) = 2T(n / 2) + O(1)$ & $O(n)$ & 1, 2, 4, ... n: Sum of GP\\
        \midrule
        $T(n) = T(n / 2) + O(n)$ & $O(n)$ & n, n/2, n/4 ... 1: Sum of GP\\ 
        \bottomrule
    \end{tabular}

    \subsection{AP GP Sums}

    \begin{tabular}{l}
        \toprule
        For AP, $S_{n} = \frac{1}{2}n(a_{1} + a_{n})$\\
        \tabitem If AP is 1, 2,...n, $S_{n} = \frac{n^{2} + n}{2} = O(n^{2})$\\
        \midrule
        For GP, $S_{n} = \frac{a(r^{n}-1)}{r-1} = \frac{a(1 - r^{n})}{1- r}$\\
        \tabitem Sum to $\infty$ $S_{\infty} = \frac{a}{1-r}$\\
        \tabitem If GP is 1, 2, 4...n, where a = n, $r = 1/2$, $S_{n} = \frac{a}{1-r} = \frac{n}{1-0.5} = O(n)$\\
    \end{tabular}
    
    \pagebreak

    \section{Trees}

    Data Structure: Implementing a Dictionary, for eg

    \subsection{Binary (Search) Trees}

    \begin{tabular}{l}
        \tabitem Binary Tree is either: 1) Empty, 2) A node pointing to 2 binary trees.\\
        \tabitem Binary Search Trees: \textbf{All in left sub-tree} $<$ key $<$ \textbf{All in right sub-tree}\\
        \tabitem \textbf{Binary Tree} is height balanced if every node in the tree is height-balanced.\\
        \tabitem A height-balanced tree with n nodes has height $h < 2log(n)$, $\therefore O(logn)$.\\
    \end{tabular}
    \bigskip

    \noindent\begin{tabular}{ll}
        \textbf{Time Complexity of search(key) in BST:} & Height of tree\\
        & \tabitem $\bm{O(logn)}$ if balanced\\
        & \tabitem Else, worst-case $\bm{O(n)}$\\
    \end{tabular}

    \subsection{Tree Traversal}

    \begin{tabular}{ll}
        \textbf{In-Order:} & Visit left sub-tree, then SELF, then right sub-tree\\
        \textbf{Pre-Order:} & Visit SELF, then left sub-tree, then right sub-tree\\
        \textbf{Post-Order:} & Visit left sub-tree, then right sub-tree, then SELF\\
        \textbf{Level-Order} & Visit EVERY node at that height, then go lower level\\
        \multicolumn{2}{l}{$\bm{O(n)}$ \textbf{Time Complexity} ($\because$ Visit each node once)}\\
    \end{tabular}

    \subsection{Successor Finding}

    \begin{tabular}{ll}
        \textbf{Basic Strategy: successor(key)} & 1. Search for key\\
        &2. If $(result > key)$, then return result.\\
        &3. If $(result \leq key)$, then search for successor
        of result.\\
    \end{tabular}

    \begin{verbatim}
    //Search for the successor of the current TreeNode
    public TreeNode successor(){
        if (rightTree != null) return rightTree.searchMin();

        TreeNode parent = parentTree;
        TreeNode child = this;
        while ((parent != null) && (child == parent.rightTree))
            child = parent;
            parent = child.parentTree;
        }
        
        return parent;
    }
    \end{verbatim}

    \begin{itemize}
        \item $\bm{O(height)}$ \textbf{Time Complexity}
    \end{itemize}


    \pagebreak

    \subsection{Insertion/Deletion}

    Insertion trivial:
    
    If less than node, node.left == null, insert at left else recurse left. 
    
    If more than node, node.right == null, insert at right, else recurse right.\\\\


    \noindent\begin{tabular}{ll}
        \toprule
        \textbf{3 Cases for delete(v):} & \\
        \textbf{No Children}& Remove v\\
        \hline
        \textbf{1 Child}& Remove v, connect child(v) to parent(v)\\
        \hline
        \textbf{2 Children}& 1. x = successor(v)\\
        & 2. delete(x) (which may cause more calls of delete)\\
        & 3. remove(v)\\
        & 4. connect x to left(v), right(v), parent(v)\\
        \bottomrule
    \end{tabular}\\\\

    \begin{itemize}
        \item \textbf{NOTE: Successor of deleted node has at most 1 child!} (A right node)
        \item $\bm{O(height)}$ \textbf{Time Complexity} (BOTH insertion and deletion)
    \end{itemize}

    \subsection{Balance}

    \textbf{A BST is balanced if }$\bm{h = O(log n)}$

    \bigskip
    
    \noindent\begin{tabular}{ll}
        \textbf{How to get a Balanced Tree:}\\
        1. Define good property of tree & [AUGMENT]\\
        2. Show that if property holds, tree is balanced. & [DEFINE BALANCE CONDITION]\\
        3. Every insertion/deletion, make sure good property still holds: & \textbf{[INVARIANT]}\\
        -If not, fix it & [MAINTAIN BALANCE]\\
    \end{tabular}

    \pagebreak

    \subsection{AVL Trees}

    \begin{tabular}{l}
        \tabitem Every node, store height $h = max(left.height, right.height) + 1$ \\
        \tabitem On insert \& delete, update height\\
        \tabitem node v is height-balanced if $\bm{|v.left.height - v.right.height| \leq 1}$\\
        \tabitem Maintains balance using Tree-Rotations\\
        \tabitem Max height $\bm{h < 2logn}$, $\bm{n > 2^{h/2}}$
    \end{tabular}

    \subsubsection{Rotations}

    \begin{tabular}{l}
        \tabitem A is LEFT-heavy if left.height $>$ right.height\\
        \tabitem A is RIGHT-heavy if right.height $>$ left.height.\\
    \end{tabular}

    \bigskip

    \noindent\begin{tabular}{ll}
        \toprule
        \textbf{Assuming node v is Left-Heavy}\\
        \hline
        \hline
        \tabitem v.left is balanced: & \hyperref[rightrot]{right-rotate(v)}\\
        \hline
        \tabitem v.left is left-heavy: & \hyperref[rightrot2]{right-rotate(v)}\\
        \hline
        \tabitem v.left is right-heavy: & 1. \hyperref[case31]{left-rotate(v.left)}\\
        & 2. \hyperref[case32]{right-rotate(v)}\\
        \bottomrule
        If v is \textbf{Right-Heavy: } & \textbf{Symmetric 3 cases}\\
        \bottomrule
    \end{tabular}

    \bigskip

    Size of tree doesn't matter, $\therefore O(1)$ time.
    
    \subsubsection{Insertion}

    \begin{tabular}{l}
        1. Insert tree in BST \\
        2. Walk up tree:\\
        \tabitem At every step, check for balance:\\
        \tabitem If out-of-balance, use rotations to rebalance\\    
        Only need \textbf{2 Rotations} (Since in all cases, only need to reduce height of sub-tree by 1)\\  
    \end{tabular}
    

    \subsubsection{Deletion}

    \begin{tabular}{l}
        0a. If v has no child, just delete \\
        0b. If v has 1 child, connect child to parent\\
        1. If v has 2 children, swap it with its successor.\\
        2. Delete node v from binary tree (and reconnect children)\\
        \tabitem Since successor has at most 1 (right) child, will only have to reconnect 1 node\\
        3. For every ancestor of the deleted node:\\
        \tabitem Check if it is height-balanced\\
        \tabitem If not, perform a rotation\\
        \tabitem Continue to the root\\
        (\textbf{Deletion may take up to O(logn) rotations})\\        
    \end{tabular}


    \pagebreak
    
    \subsubsection{Graphical Interpretation}

    \begin{figure}[htbp]
        \begin{center}
        \includegraphics[width=5in,keepaspectratio]{Case 1 Right Rotate.JPG}
        \caption{v.left balanced: right-rotate(v)}
        \label{rightrot}
        \end{center}
    \end{figure}

    \begin{figure}[htbp]
        \begin{center}
        \includegraphics[width=5in,keepaspectratio]{case 2 right rotate.JPG}
        \caption{v.left left-heavy: right-rotate(v)}
        \label{rightrot2}
        \end{center}
    \end{figure}

    \begin{figure}[htbp]
        \begin{center}
        \includegraphics[width=5in,keepaspectratio]{Case 3.1.JPG}
        \caption{v.left right-heavy: First left-rotate(v.left)}
        \label{case31}
        \end{center}
    \end{figure}

    \begin{figure}[htbp]
        \begin{center}
        \includegraphics[width=5in,keepaspectratio]{Case 3.2.JPG}
        \caption{v.left right-heavy: then right-rotate(v)}
        \label{case32}
        \end{center}
    \end{figure}


    \pagebreak

    \section{Other (Augmented) Trees}
    \subsection{Tries}
    Store each letter of a String as a node, using a special flag to represent the end of a word.

    \begin{tabular}{l}
        Cost to search a string of length L: $\bm{O(L)}$\\
        Trie tends to be faster compared to normal BST with strings\\
        \tabitem Does not depend on size of total text\\
        \tabitem Does not depend on number of strings (Esp if string not in trie)\\
        Trie uses more space (in terms of more nodes)
    \end{tabular}
    
    \subsection{Order Statistics}

    \begin{tabular}{l}
        \tabitem To know the order of the node (ie rank of the key in the data structure)\\
        \tabitem Store \textbf{size of sub-tree in every node}\\
        \tabitem select(k): finds node with rank k\\
        \tabitem rank(v): Computes rank at node v\\
        \tabitem During insertion, maintain weight during \hyperref[orderstats]{rotation}
    \end{tabular}

    \begin{verbatim}
        select(k)
            rank = left.weight + 1;
            if (k == rank) then
                return v;
            else if (k < rank) then
                return left.select(k);
            else if (k > rank) then
                return right.select(k minus rank);
    \end{verbatim}

    \begin{verbatim}
        rank(node)
            rank = node.left.weight + 1;
            while (node != null) do
                if node is left child then
                    do nothing
                else if node is right child then
                    rank += node.parent.left.weight + 1;
                node = node.parent;
            return rank;
    \end{verbatim}

    \begin{figure}[htbp]
        \begin{center}
        \includegraphics[width=3in,keepaspectratio]{orderstats.JPG}
        \caption{Update weights during insertion}
        \label{orderstats}
        \end{center}
    \end{figure}

    \pagebreak

    \subsection{Interval Trees}

    \begin{tabular}{l}
        \textbf{Find an interval containing a value}\\
        \tabitem Each node is an interval, sorted by \textbf{left endpoint}\\
        \tabitem Each node contains the \textbf{maximum endpoint in subtree}\\
        \tabitem Running time of search simply $O(logn)$\\
    \end{tabular}

    \begin{verbatim}
        //Find interval containing x
        interval-search(x)
            c = root;
            while (c != null and x is not in c.interval) do
                if (c.left == null) then
                    c = c.right;
                else if (x > c.left.max) then
                    c = c.right;
                else c = c.left;
            return c.interval;
    \end{verbatim}

    \bigskip

    \begin{tabular}{l}
        Search find an overlapping interval, if it exists.\\
        \tabitem If search goes right: No overlap in left-subtree\\
        $\therefore$ \textbf{key is in right subtree or it is not in tree}\\
        \tabitem If search goes left and no overlap, then key $<$ every interval in right sub-tree.\\
        $\therefore$ \textbf{Either finds key in left subtree or it is not in the tree}\\
    \end{tabular}

    \pagebreak 

    \subsection{Range Trees/Orthogonal Range Searching}

    \begin{tabular}{l}
        \textbf{Find everyone between a certain range}\\
        \tabitem Stores all points in the \textbf{leaves} (Internal nodes store copies)\\
        \tabitem Internal node v stores \textbf{max(v.left)}\\
        \tabitem First find the 'split node': Is node between specified range?\\
        $\therefore$ Do both Left and Right traversal at split node to get all nodes within range\\
    \end{tabular}

    \begin{verbatim}
        FindSplit(low, high)
            v = root;
            done = false;
            while !done {
                if (high <= v.key) then v=v.left;
                else if (low > v.key) then v=v.right;
                else (done = true);
            }
            return v;
    \end{verbatim}

    \begin{verbatim}
        RightTraversal(v, low, high)
            if (v.key <= high) {                //Still within range
                all-leaf-traversal(v.left);
                RightTraversal(v.right, low, high);
            } else {                           //Left max larger than range, just go left
                RightTraversal(v.left, low, high);
            }
    \end{verbatim}

    \begin{verbatim}
        LeftTraversal(v, low, high)
            if (low <= v.key) {                 //Still within range
                all-leaf-traversal(v.right);
                LeftTraversal(v.left, low, high);
            } else {                            //Left max smaller than range, just go right
                LeftTraversal(v.right, low, high);
            }
    \end{verbatim}

    \bigskip

    \begin{tabular}{l}
        \tabitem Finding split node: $O(logn)$\\
        \tabitem Traversals recurse at most $O(logn)$ times, \\outputting all (all-leaf-traversal())
        is $\bm{O(k)}$, where k is number of items found.\\
        \tabitem $\therefore$ \textbf{Query time complexity } $\bm{ = O(logn + k)}$\\
        \tabitem Preprocessing (buildtree) time complexity: $O(nlogn)$\\
        (Split into left and right, take highest value of left and put as key\\
        If numofelements==1, then set as leaf)\\
        \tabitem Space Complexity: $O(n)$\\
        \tabitem \emph{If just want to know the count: keep count of num of nodes in each sub-tree,} \\
        \emph{and retreive that instead of all-leaf-traversal.}\\
    \end{tabular}

    \bigskip

    \noindent Related: kd-trees (k-dimension)



    \pagebreak

    \section{Hashing}
    
    Standard symbol table supports:

    \begin{tabular}{l}
        \tabitem void insert(key, value)\\
        \tabitem value search(key)\\
        \tabitem void delete(key)\\
        \tabitem bool contains(key)\\
        \tabitem int size()\\\\
    \end{tabular}


    Costs of \textbf{Search} and \textbf{Insert/Delete}, and other functions required: See specifications

    \begin{tabular}{l}
        \tabitem AVL Tree: $O(logn)$ each\\
        \tabitem Symbol Table: $\bm{O(1)}$ each, but extra functionality, eg Sorting ($O(nlogn)$ vs $O(n^{2}$)\\
        \tabitem Symbol Table also no prede/successor queries
        \tabitem \textbf{Since Symbol Tables are not comparison-based}\\
    \end{tabular}

    \subsection{Hash Functions \& Collisions}

    Direct Access Tables take too much space (Number of possible keys very large)\\

    \noindent\textbf{Map keys to buckets using Hash Functions}\\

    \noindent\emph{Assume m buckets, n entries, and h is the hash function,}\\
    
    \begin{tabular}{l}
        \tabitem 2 distinct keys \textbf{collide} if: $h(k_{1}) = h(k_{2}))$\\
        \tabitem Collisions \textbf{unavoidable} by Pigeonhole Principle (Table Size $<$ Universe Size)\\
    \end{tabular}


    \pagebreak

    \subsection{Collision Handling: Chaining}

    

    Put both items in same bucket, using linked List of items.
    \newline

    \begin{tabular}{ll}
        \toprule
        \textbf{Total Space:} & $O(m + n)$\\
        \midrule
        \textbf{Insertion:} & Find hash value, add to head of linked list\\
        &$\therefore O(1 + cost(h))$\\
        \midrule
        \textbf{Search:} & Find hash value, search through linked list\\
        &Worst case all values go to same bucket (emphasizing importance of good hash function)\\
        &$\therefore O(n + cost(h))$\\
        \bottomrule
    \end{tabular}

    \subsubsection{Simple Uniform Hashing Assumption}

    Assume "random" mapping:

    \begin{tabular}{l}
        \tabitem Every key is equally likely to map to every bucket\\
        \tabitem Keys mapped independently\\
        \tabitem $\therefore$ \textbf{As long as enough buckets, won't get too many keys in one bucket}\\\\
    \end{tabular}

    \begin{tabular}{l}
        If $X(i, j) = 1$ if item i is put in bucket j, and $0$ otherwise,\\
        \tabitem $P(X(i, j) == 1) = 1/m$\\
        \tabitem $E(X(i, j)) = 1/m$\\
    \end{tabular}

    \begin{tabular}{ll}
        \tabitem Thus, expected number of items per bucket &= $E(\Sigma_{i}X(i, b))$\\
        &= $\Sigma_{i}E(X(i, b))$\\
        &= $\Sigma_{i}1/m$\\
        &= $n/m$\\
    \end{tabular}

    \begin{tabular}{l}
        \tabitem $\therefore$ \textbf{load(hashtable)} = average number of items per bucket $\bm{= n/m}$\\\\
    \end{tabular}

    Therefore, for a Hashtable with chaining under SUHA assumption:\\

    \begin{tabular}{ll}
        \toprule
        \textbf{Search time:} & $1+ n/m$ (Hash function + linked list traversal)\\
        \tabitem Expected & $O(1)$  (Assuming $m = \Omega(n)$ buckets, eg $m = 2n$)\\
        \tabitem Worst-case & $O(n)$\\
        \midrule
        \textbf{Worst-Case Insertion:} & $O(1)$ if allow duplicates, preventing duplicate requires searching\\
        \midrule
        \textbf{Expected max linked-list length/cost} & $O(logn)$ or $\Theta(logn/loglogn)$\\
        \bottomrule
    \end{tabular}

    \pagebreak

    \subsection{Collision Handling: Open-Addressing}

    
    \begin{tabular}{l}
        \tabitem All data directly stored in the table, one item per slot.\\
        \tabitem On collision, \textbf{probe sequence of buckets until empty one found}\\
        \tabitem When $m == n$, \textbf{table is full, cannot insert any more items}; cannot search efficiently\\
        \tabitem Redefined Hash Function: $h(key, i)$, where i = number of collisions\\
        \tabitem \textbf{Linear Probing:} Keep checking the next bucket, $h(k, 1) + ($i $mod$ m$)$\\
    \end{tabular}


    \begin{verbatim}
        hash-insert(key, data)
        int i = 1;
        while (i <= m):                         // Try every bucket
            int bucket = h(key, i);
            if (T[bucket] == null):             // Found an empty bucket
                T[bucket] = {key, data};        // Insert key/data
                return success;                 // Return
            i++;
        throw new TableFullException();         // bucket full
    \end{verbatim}

    \begin{verbatim}
        hash-search(key)
            int i = 1;
            while (i <= m):
                int bucket = h(key, i);
                if (T[bucket] == null) return key-not-found;        // Empty bucket!
                if (T[bucket].key == key) return T[bucket].data;    // Full bucket
                i++;
            return key-not-found;                                   // Exhausted entire table.
    \end{verbatim}

    \begin{tabular}{l}
        \textbf{delete(key):} Find key to delete, \textbf{set bucket to DELETED (A tombstone value)}\\
        \tabitem Cannot set as NULL, since search may then fail to find a key after that bucket.\\
        \tabitem When insert(key) comes to DELETED, \textbf{overwrite deleted cell}.\\\\
    \end{tabular}

    \begin{tabular}{l}
        
    \end{tabular}

    \subsubsection{Properties of good Hash Functions}

    
    \begin{tabular}{l}
        \textbf{1.} $\bm{h(key, i)}$ \textbf{enumerates all possible buckets}\\
        \tabitem $\forall$ bucket $j, \exists$ i $: h(key, i) = j$\\
        \tabitem The hash function is permutation of ${1...m}$\\
        \tabitem If not, may return table-full when still have space left\\\\
    \end{tabular}
    
    \noindent\begin{tabular}{l}
        \textbf{2. Uniform Hashing Assumption}\\
        \tabitem Every key is equally likely to be mapped to every \textbf{permutation of buckets}, independent of every other key.\\
        \tabitem Linear Probing does NOT fulfill this criteria: \textbf{Clustering} can reach $\Theta(logn)$, ruins constant time performance\\
    \end{tabular}

    \emph{In practice though, linear probing is desirable due to caching}

    \noindent\begin{tabular}{l}
        \tabitem\textbf{Achieved through double hashing}\\
        \tabitem Using 2 hash functions $g(k), f(k)$, $\bm{h(k, i) = [f(k) + ig(k)]}$ $\bm{mod}$ \textbf{m} for some large m\\
    \end{tabular}

    \emph{Specifically, if g(k) is relatively prime to m, then h(k, i) hits all buckets}

    \pagebreak

    \subsubsection{Performance of Open Addressing}
    \noindent\begin{tabular}{l}
        \textbf{Expected Cost = First Probe + P(collision on first probe) * Expected Cost of remaining probes}\\
        \tabitem $= 1 + (n/m)(...)$\\
        \tabitem $= 1 + (n/m)(1 + [n-1/m-1][...])$\\
        \tabitem $\leq 1 + \alpha(1 + \alpha(...))$\\
        \tabitem $\leq 1 + \alpha + \alpha^{2} + \alpha^{3} + ...$\\
        \tabitem $\leq \frac{1}{1 - \alpha}$\\\\
    \end{tabular}

    \noindent\begin{tabular}{l}
        \textbf{Advantages}\\
        \tabitem Saves space\\
        \tabitem Rarely Allocate Memory\\
        \tabitem Better Cache performance\\
        \textbf{Disadvantages}\\
        \tabitem More sensitive to choice of hash functions\\
        \tabitem More sensitive to load (as $\alpha \rightarrow 1$)
    \end{tabular}

    \subsection{Resizing}

    \noindent\begin{tabular}{l}
        Assume\\
        \tabitem Hashing with Chaining\\
        \tabitem SUHA\\
        \textbf{Expected Search Time:} $O(1 + n/m)$\\
        \textbf{Optimal Size:} $m = O(n)$\\\\
    \end{tabular}

    If m too big ($> 10n$), too much wasted space; if m too small ($< 2n$), too many collisions\\
    
    \noindent\begin{tabular}{l}
        \textbf{To expand hashtable:}, let $m_{1} and m_{2}$ be old and new hashtable size\\
        \tabitem Scan old hash table: $O(m_{1})$, Initialise new table: $O(m_{2})$\\
        \tabitem Insert each element in new hashtable: $O(1) * n$\\
        \tabitem \textbf{Total: } $\bm{O(m_{1} + m_{2} + n)}$\\
        \tabitem If double table size, $(n == m), m = 2m$: $O(n)$ time\\\\
    \end{tabular}

    \noindent\begin{tabular}{l}
        \textbf{To shrink hashtable:}, let $m_{1} and m_{2}$ be old and new hashtable size\\
        \tabitem Cannot be same ratio as insert, cos there will be a point where deleting/inserting 1 shrinks/expands the table\\
        If insert doubles the table, then for delete:\\
        \tabitem If $(n < m/4), m = m/2$\\\\
    \end{tabular}

    \noindent\begin{tabular}{l}
        \textbf{Costs of operations:}\\
        \tabitem Inserting k elements costs $O(k)$\\
        \tabitem $\therefore$ Insert operation: \textbf{Amortized }$\bm{O(1)}$\\
        \tabitem Search operation: \textbf{Expected }$\bm{O(1)}$\\\\
    \end{tabular}

    \pagebreak

    \section{Sets}

    insert(Key k), contains(Key k), delete(Key k), intersect(Set<Key> s), union(Set<Key> s)

    \subsection{Implementation using Hashtable}

    Takes more space to keep the entire key (to resolve collisions) in the table.

    \subsection{Fingerprint Hashtable}

    \noindent\begin{tabular}{l}
        Stores bits (0 and 1) instead of the key, 0 if not present, 1 if present. No key stored in the table.\\
        \tabitem \textbf{Collisions possible}\\
        \tabitem Lookup operation: If key is \textbf{in}, will always report true (\textbf{No False Negatives})\\
        \tabitem Due to collisions, even in key not in set, may sometimes report true (\textbf{False Positives})\\
        Thus choosing what to store is important, based on objectives\\
    \end{tabular}

    \subsubsection{Table Size vs P(False Positives)}

    \begin{tabular}{l}
        On a lookup of n elements of table of size m,\\
        \tabitem P(No false positive) = $(1 - 1/m)^{n} \approx (1/e)^{n/m}$\\
        \tabitem P(False positive) = $1 - (1/e)^{n/m}$\\\\
        Assuming we want P(false positive) at most p:\\s
        \tabitem $n/m \leq log(\frac{1}{1-p})$\\
        So we reduced space to 1 bit per slot, but need a bigger table to avoid collisions\\\\
    \end{tabular}
    
    \subsection{Bloom Filter}

    \begin{tabular}{l}
        Fingerprint Hashtable, but 2 hash function to \textbf{store 1 in 2 different slots}.\\
        \tabitem Lookup: Check if both slots are 1\\
        \tabitem Still,\textbf{No False Negatives} and possible \textbf{False Positives}\\
        Requires 2 collisions to be a false positive, but each item take more space.\\\\
        Assuming we want P(false positive) at most p:\\
        \tabitem $n/m \leq \frac{1}{2}log(\frac{1}{1-p^{1/2}})$\\\\
        Deleting elements? Consider a counter instead of 1 bit in each slot:\\
        \tabitem On insert, counter++\\
        \tabitem On delete, counter--\\
        If counter gets too big, no space saving: Thus need to make collisions rare\\\\
        Implementing Set functions:\\
        \tabitem Insert, delete, query: $O(k)$\\
        \tabitem Intersection, Bitwise AND 2 bloom filters: $O(m)$\\
        tabitem Union, Bitwise OR 2 bloom filters: $O(m)$\\
    \end{tabular}

    \pagebreak

    \section{Other Data Structures}

    \subsection{(a, b)-trees and B-trees}

    \begin{tabular}{l}
        \toprule
        \textbf{a, b refer to min and (max + 1) no. of children in node, where }$\bm{2 \leq a \leq (b+1)/2}$\\
        \midrule
        Non-leaf node must have one more child than its number of keys, its \textbf{key range:}\\
        \tabitem Keys in sorted order, $v_{1}, v_{2},...v_{k}$\\
        \tabitem First child has key range $\leq v_{1}$\\
        \tabitem Final child has key range $> v_{k}$\\
        \tabitem All other children $c_{i}$, where $i \in [2, k]$ have key range $(v_{i-1}, v_{i}]$\\
        \midrule
        All leaf nodes must be same depth\\
        \midrule
        \textbf{Insert:} split node if contain b-1 keys (Node too big)\\
        \midrule
        \textbf{Delete:} if deleting make node too small, merge siblings y,z if have total nodes $\leq b-1$, \\
        else share by merging and splitting\\
        \bottomrule\\
    \end{tabular}

    \noindent\textbf{B-trees} are (a, b)-trees such that $a = B$, $b = 2B$

    \begin{figure}[htbp]
        \begin{center}
        \includegraphics[width=5in,keepaspectratio]{Btree.JPG}
        \caption{B-tree, where B = 2}
        \end{center}
    \end{figure}

    \subsection{Skip Lists}

    \subsection{Merkle Trees}

    \pagebreak

    \section{Graphs}

    \begin{tabular}{l}
        Consists of at least 1 node, and unique edges that connect 2 nodes\\
        \textbf{Hypergraph}: Eaah unique edge connect $\geq 2$ nodes\\
        \textbf{Multigraph}: Each node connected by more than 1 edge\\
        Degree of node: Number of adjacent edges\\
        Degree of graph: max(degree of nodes)\\
        Diameter: Max distance between 2 nodes, following shortest path\\
        \textbf{Bipartite graph}: Nodes divided to 2 sets, no edges between same set\\
    \end{tabular}

    \subsection{Adjacency list}

    \begin{tabular}{l}
        Nodes stored in an array, Edges stored as linked list per node\\\\
        \textbf{Memory Usage: }$\bm{O(V + E)}$, since of array V and size of linked lists E\\\\
        Are v and w neighbours? \textbf{Fast query}\\
        Find any neighbour of v: \textbf{Slow query}\\
        Enumerate all neighbours: \textbf{Slow query}\\
    \end{tabular}

    \subsection{Adjacency Matrix}

    \begin{tabular}{l}
        Edges seen as pairs of nodes. For a graph with n nodes, nxn array:\\
        At A[i][j], 1 if i and j are directly connected\\
        $A^{n}$: Length of n paths\\\\
        \textbf{Memory Usage:} $\bm{O(V^{2})}$\\\\
        Are v and w neighbours? \textbf{Slow query}\\
        Find any neighbour of v: \textbf{Fast query}\\
        Enumerate all neighbours: \textbf{Fast query}\\\\
    \end{tabular}

    \noindent\textbf{Generally, if graph is dense, use an adjacency matrix, if not then adjacency list}

    \pagebreak
    
    \section{Graph Traversal}

    Start at vertex s, ends at vertex t, or visit all nodes in the graph. (Assume adjacency list)
    

    \subsection{Breadth-First Search}

    \begin{tabular}{l}
        \tabitem \textbf{Finds shortest path}\\
        \tabitem Skip already visited nodes, calculate level[i] from level[i-1]\\
    \end{tabular}

    \begin{verbatim}
    //Or can use a QUEUE to pop the earlier ones first

    BFS(Node[] nodeList) {
        boolean[] visited = new boolean[nodeList.length];
        Arrays.fill(visited, false);

        int[] parent = new int[nodelist.length];
        Arrays.fill(parent, -1);

        // To make sure you visit all components 
        for (int start = 0; start < nodeList.length; start++) {
            if (!visited[start]){
                Bag<Integer> frontier = new Bag<Integer>;
                frontier.add(startId);

                // Main code
                while (!frontier.isEmpty()){
                    Collection<Integer> nextFrontier = new … ;
                    for (Integer v : frontier) {
                        for (Integer w : nodeList[v].nbrList) {
                            if (!visited[w]) {
                                visited[w] = true;
                                parent[w] = v;
                                nextFrontier.add(w);
                            }
                        }
                    }
                    frontier = nextFrontier;
                }
            }
        }
    }
    \end{verbatim}

    \begin{tabular}{l}
        \textbf{Running Time:} $\bm{O(V + E)}$\\
        \tabitem Every vertex v = start once, and added to nextFrontier once \\
        (After visited, never re-added: $O(V)$)\\
        \tabitem Each v.nbrList enumerated once: $O(E)$\\\\
        Shortest path is a tree - Parent pointers store shortest path\\\\
        \textbf{Does NOT explore every path in the graph!!!}\\ 
    \end{tabular}

    \pagebreak

    \subsection{Depth-first search}

    \begin{tabular}{l}
        \tabitem Follow path until end, backtrack until find new edge, recursively explore
        \tabitem Skip already visited nodes
    \end{tabular}

    \begin{verbatim}
        // Iterative method would be to use a STACK

        DFS(Node[] nodeList){
            boolean[] visited = new boolean[nodeList.length];
            Arrays.fill(visited, false);

            for (start = i; start<nodeList.length; start++) {
                if (!visited[start]){
                    visited[start] = true;
                    DFS-visit(nodeList, visited, start);
                }
            }
        }

        DFS-visit(Node[] nodeList, boolean[] visited, int startId){
            for (Integer v : nodeList[startId].nbrList) {
                if (!visited[v]){
                    visited[v] = true;
                    DFS-visit(nodeList, visited, v);
                }
            }
        }
    \end{verbatim}

    \begin{tabular}{l}
        \textbf{Running Time:} $\bm{O(V + E)}$\\
        \tabitem Each node is visited only once: $O(V)$\\
        \tabitem For every node, each neighbour is enumerated: $O(E)$\\\\
        Running time for adjacency matrix: $O(V^{2})$, calls once per node at O(V), enumerates neighbours at O(V)\\\\
    \end{tabular}

    \subsection{Problems with BFS and DFS}

    \begin{itemize}
        \item Do not visit every path in the graph
        \item Too expensive for graphs with exponential number of paths
    \end{itemize}

    \pagebreak

    \subsection{Directed Graphs}

    \begin{tabular}{l}
        \textbf{In-degree}: Number of incoming edges\\
        \textbf{Out-degree}: Number of outgoing edges\\\\
        \textbf{Memory Usage in Adjacency List: }$\bm{O(V + E)}$, where ll stores outgoing edges\\
        \textbf{Memory Usage in Adjacency Matrix:} $\bm{O(V^{2})}$, where A[v, w] represent edge from v to w\\\\
        Are v and w neighbours? \textbf{Slow query}\\
        Find any neighbour of v: \textbf{Fast query}\\
        Enumerate all neighbours: \textbf{Fast query}\\\\
    \end{tabular}

    \subsection{Topological Ordering}

    \begin{tabular}{l}
        Sequential total ordering of all nodes, edges only point forward.\\
        Use \textbf{post-order} DFS: Process node when it is last visited\\
        Topological Ordering is NOT unique\\
        \textbf{Time Complexity: }$O(V+E)$\\
    \end{tabular}

    \begin{verbatim}
        DFS(Node[] nodeList){
            boolean[] visited = new boolean[nodeList.length];
            Arrays.fill(visited, false);
            for (start = i; start<nodeList.length; start++) {
                if (!visited[start]){
                    visited[start] = true;
                    DFS-visit(nodeList, visited, start);
                    schedule.prepend(v);
                }
            }
        }
    \end{verbatim}

    \begin{tabular}{l}
        Alternatively, \textbf{Kahn's Algorithm}\\
        Repeat:\\
        \tabitem S = nodes in G that have no incoming edges.\\
        \tabitem Add nodes in S to the topo-order\\
        \tabitem Remove all edges adjacent to nodes in S\\
        \tabitem Remove nodes in S from the graph\\
        \textbf{Time Complexity: }$O(V+E)$, or $O(ElogV)$ using a PQ\\
    \end{tabular}

    \subsection{Shortest Path in a Directed Acyclic Graph}

    \begin{tabular}{l}
        Relax the edges in the right-order: Relax each edge once, $O(E)$ cost for relaxation step\\
        DFS post-order, find in topological order\\\\
        Running time of Shortest Path on a DAG: $O(E)$\\
        \textbf{Longest Path: Shorted path in negated graph or Modify relax function}\\\\
        Longest path in a general cyclic graph is NP hard\\
    \end{tabular}

    \subsection{Shortest path in a tree}

    \begin{tabular}{l}
        From source to destination, only 1 possible path.\\
        From source to all? \textbf{BFS or DFS order}\\\\
        Running time: $O(V)$, assuming weighted undirected tree\\
        $\because$ there are only O(V) edges in the tree.
    \end{tabular}


    \pagebreak

    \subsection{Single-Source Shortest Paths of Weighted directed Graphs}

    \begin{tabular}{l}
        Cannot use BFS: BFS finds minimum hops from node to node, not minimum distance (of weighted edges)\\
        \textbf{Triangle Inequality:} $\bm{\delta(S, C) \leq \delta(S, A) + \delta(A, C)}$\\
        Mantain estimate for each distance, reduce estimate if a lower value is found by \textbf{relaxing edges}.\\
        \textbf{Invariant:} estimate $\leq$ distance\\
    \end{tabular}

    \subsubsection{Bellman-Ford}

    Simple, general way to find SSSP

    \begin{verbatim}
        int[] dist = new int[V.length];
        Arrays.fill(dist, INFTY);
        dist[start] = 0;

        // Bellman-Ford:
        // Relax every edge |V| times, stop when converges
        n = V.length;
        for (i=0; i<n; i++)
            for (Edge e : graph)
                relax(e)

        // Not stated here, but can terminate early
        // once an entire sequence of E relax operations have no effect
        // (ie when one inner for-loop doesn't change anything)

        relax(int u, int v){
            if (dist[v] > dist[u] + weight(u,v))
                dist[v] = dist[u] + weight(u,v);
        }       
    \end{verbatim}

    \begin{tabular}{l}
        \textbf{Running Time:} $O(EV)$\\
        $\because$ Outer for-loop is $O(V)$, inner is $O(E)$\\\\
        Negative Weight: Possibility of \textbf{Negative Weight Cycles}\\
        \tabitem To detect: \textbf{Run Bellman-Ford for }$|V| + 1$ \textbf{iterations}\\\\
        If all edges have same weight: Use regular BFS (Distance no different from hops)\\
    \end{tabular} 

    \pagebreak

    \subsubsection{Dijkstra}

    
    \begin{tabular}{l}
        Faster, \textbf{only non-negative weights}, takes edge from vertex closest to source.\\
        1) Maintain distance estimate for every node.\\
        2) Begin with empty shortest-path-tree\\
        3) Repeat: \\
        \tabitem Consider vertex with minimum estimate\\
        \tabitem Add vertex to shortest-path-tree\\
        \tabitem Relax all outgoing edges\\
        Use of \textbf{Priority Queue via AVL Tree}\\
        \textbf{Every finished vertex has a good estimate; Initially, only start is finished}\\
        \textbf{This does NOT hold with negative edge weights}\\
    \end{tabular}

    \begin{verbatim}
        public Dijkstra{
            private Graph G;
            private IPriorityQueue pq = new PriQueue();
            private double[] distTo;

            searchPath(int start) {
                pq.insert(start, 0.0);
                distTo = new double[G.size()];
                Arrays.fill(distTo, INFTY);
                distTo[start] = 0;
                while (!pq.isEmpty()) {
                    int w = pq.deleteMin();
                    for (Edge e : G[w].nbrList)
                        relax(e);
                }
            }

            // Relax now decreases key in priority queue if needed
            relax(Edge e) {
                int v = e.from();
                int w = e.to();
                double weight = e.weight();
                if (distTo[w] > distTo[v] + weight) {
                    distTo[w] = distTo[v] + weight;
                    parent[w] = v;
                    if (pq.contains(w)) pq.decreaseKey(w, distTo[w]);
                    else pq.insert(w, distTo[w]);
                }
            }
        }
    \end{verbatim}
        
    \begin{tabular}{l}
        Assuming AVL Tree priority queue:\\
        \tabitem insert/push, deleteMin/pop, decreaseKey: $O(logn)$\\
        \tabitem contains: $O(1)$\\
        insert/deleteMin: $|V|$ times each, since each node added to PQ only once\\
        relax/decreaseKey: $|E|$ times, since each edge is relaxed once\\\\
        $\therefore$\textbf{ Running time: }$\bm{O((V+E)logV) = O(ElogV)}$\\
        (Running time with array and heap: $O(V^{2})$ and $O(ElogV)$)\\\\
        \textbf{Source-to-Destination Djisktra: }\\
        Can choose to terminate once destination is dequeued, since it is a good estimate\\
    \end{tabular}

    \section{Heaps}

    \begin{tabular}{l}
        Maintain set of prioritized object\\
        \tabitem used for stuff like PQ: insert, extractMax, increase/decreaseKey, delete\\
        \tabitem Unlike AVL, no rotations\\\\
        \textbf{2 Properties: }\\
        \tabitem \textbf{Heap Ordering}: priority[parent] $\geq$ priority[child]\\
        \tabitem \textbf{Complete Binary Tree}, nodes as far left as possible\\\\
        Biggest items stored at root, smallest at leaves\\
        Maximum Height: \textbf{floor(logn)} $\bm{= O(logn)}$\\
    \end{tabular}

    \subsection{PQ Operations}

    \begin{tabular}{ll}
        insert & Insert priority p as leaf, bubble up by swapping with parent until parent's priority larger than p.\\
        increaseKey & Update priority, bubbleUp until parent's priority larger than new priority\\
        decreaseKey & Update priority, bubbleDown (leftwards)\\
        delete & \tabitem Swap node with last() (most right value rooted at node)\\
        & \tabitem remove last()\\
        & \tabitem bubbledown original last() from prev node's position.\\
        extractMax & delete(root), return original root\\
    \end{tabular}
    \begin{tabular}{l}
        \textbf{Heap Operations are O(logn)}\\
    \end{tabular}

    \begin{verbatim}
        bubbleUp(Node v) {
            while (v != null) {
            if (priority(v) > priority(parent(v)))
                swap(v, parent(v));
            else return;
            v = parent(v);
            }
        }

        bubbleDown(Node v) {}
            while (!leaf(v)) {
                leftP = priority(left(v));
                rightP = priority(right(v));
                maxP = max(leftP, rightP, priority(v));
                if (leftP == max) {
                    swap(v, left(v));
                    v = left(v); 
                }
                else if (rightP == max) {
                    swap(v, right(v));
                    v = right(v); 
                }
                else return;
            }
        }
            
        insert(Priority p, Key k) {
            Node v = completeTree.insert(p,k);
            bubbleUp(v);
        }
    \end{verbatim}

    \pagebreak
    
    \subsection{Store heap as array}

    Map each node in complete binary tree into a slot in an array, breadth-first.

    \begin{tabular}{ll}
        insert & Append to end of the array\\
        left(x) & arr[2x + 1]\\
        right(x) & arr[2x + 2]\\
        parent(x) & floor((x - 1)/2)\\
    \end{tabular}

    \subsubsection{HeapSort: Heap array to Sorted List}

    \begin{tabular}{l}
        extractMax() n times, everything shifted to the front of the array, append max to end.\\
        \textbf{Time Complexity:} $\bm{O(nlogn)}$\\
    \end{tabular}

    \begin{verbatim}
        // int[] A = array stored as a heap
        for (int i=(n-1); i>=0; i--) {
            int value = extractMax(A); // O(log n)
            A[i] = value;
        }
    \end{verbatim}

    \subsubsection{Unsorted list to heap}

    Recurse from leaves up: Left and right childs are heaps, bubble up accordingly if not.

    \begin{verbatim}
        // int[] A = array of unsorted integers
        for (int i=(n-1); i>=0; i--) {
            bubbleDown(i, A); // O(height) = O(log n)
        }
    \end{verbatim}

    \begin{tabular}{l}
        Note that ceil(n/2) nodes are height = 0, ceil(n/4) height = 1, ... , 1 root\\
        $\therefore$ \textbf{Total cost of building heap = }$\bm{\sum_{0}^{logn}\frac{n}{2^{h}}O(h)}$,\\
        where $2^{h}$ is upper bound of nodes at level h, O(h) cost of bubbling down node at level h,\\
        $\leq cn(\frac{0.5}{(1-0.5)^{2}})$\\
        $\bm{\leq 2O(n)}$\\
    \end{tabular}

    \subsubsection{HeapSort summary}

    \begin{tabular}{l}
        Unsorted List $\rightarrow$ Heap array in O(n) $\rightarrow$ Sorted list in O(nlogn)\\
        O(nlogn) worst-case\\
        In-place; n space needed\\
        \textbf{Always completes in O(nlogn)}\\
    \end{tabular}

    \pagebreak

    \section{Union Find}
    \begin{tabular}{l}
        Given set(s) of objects,\\
        \tabitem Union - Connect two sets\\
        \tabitem Find - Are two objects in the same set?\\\\
        Transivity: If p connected to q and q connected to r, p connected to r\\
    \end{tabular}

    \subsection{Quick-find}
    \begin{tabular}{l}
        Keep array of componentIDs\\\\
        \textbf{Find:} 2 objects are connected if they have the same component identifier, \textbf{O(1) time}\\
        \tabitem If objects not integers, can use hashtable + open addressing to map items to integers instead.\\\\
        \textbf{Union:} Replace one of the IDs with the other ID, \textbf{O(n) time}\\
    \end{tabular}

    \begin{verbatim}
        find(int p, int q):
            return(componentId[p] == componentId[q]);

        union(int p, int q):
            updateComponent = componentId[q]
            for (int i=0; i<componentId.length; i++)
                if (componentId[i] == updateComponent)
                    componentId[i] = componentId[p];
    \end{verbatim}

    \subsection{Quick-Union}

    \begin{tabular}{l}
        Keep array of direct 'parent' of node\\\\
        \textbf{Find:} 2 objects are connected if they are part of the same tree, \textbf{O(n) time}\\
        \tabitem If objects not integers, can use hashtable + open addressing to map items to integers instead.\\\\
        \textbf{Union:} Attach root of one tree to the other tree, \textbf{O(n) time}\\
    \end{tabular}

    \begin{verbatim}
        find(int p, int q)
            while (parent[p] != p) p = parent[p];
            while (parent[q] != q) q = parent[q];
            return (p == q);

        union(int p, int q)
            while (parent[p] != p) p = parent[p];
            while (parent[q] != q) q= parent[q];
            parent[p] = q;
    \end{verbatim}

    \pagebreak

    \subsection{Weighted-Union}

    \begin{tabular}{l}
        Connect the smaller tree to the bigger tree; Maximum depth of tree: \textbf{O(logn)}\\\\
        Everytime a tree T of size t is linked to a tree of size t+1, total size > 2size(T)\\
        Whenever this happens, \textbf{depth of nodes in T increases by 1, since root of T linked to root of larger tree.}\\
        Max number of times size can double is up till size = n = $2^{logn}$; \textbf{Size doubles logn times}\\
        \textbf{Hence largest depth possible for a node in T is log(n)}\\
        $\therefore $\textbf{Running time of Find and Union: O(logn)}\\
    \end{tabular}

    \begin{verbatim}
        union(int p, int q)
            while (parent[p] !=p) p = parent[p];
            while (parent[q] !=q) q = parent[q];
            
            if (size[p] > size[q] {
                parent[q] = p; // Link q to p
                size[p] = size[p] + size[q];
            } else {
                parent[p] = q; // Link p to q
                size[q] = size[p] + size[q];
            }
    \end{verbatim}

    \subsection{Path Compression}

    \begin{tabular}{l}
        After finding the root: Set the parent of each traversed node as the root itself.\\
        \textbf{Time Complexity:} \\
        - \textbf{Weighted Union with Path Compression:} \\
        \tabitem Sequence of m union/find on n objects: $O(n + m\alpha(m,n))$\\
        \tabitem 1 Find/Union operation $\alpha(m,n)$\\
        - \textbf{Path Compression:} Find/Union O(logn)\\
    \end{tabular}   
    
    \begin{verbatim}
        // PREVIOUS root finding
        findRoot(int p):
            root = p;
            while (parent[root] != root) root = parent[root];
            return root;

        // Root finding with Path Compression
        findRoot(int p)
            root = p;
            while (parent[root] != root) root = parent[root];
            while (parent[p] != p):
                temp = parent[p];
                parent[p] = root;
                p = temp;
            return root;

        // Alternative: Make every OTHER node in path point to its GRANDparent
        findRoot(int p):
            root = p;
            while (parent[root] != root):
                parent[root] = parent[parent[root]];
                root = parent[root];
            return root;
    \end{verbatim}

    \pagebreak

    \section{Minimum Spanning Trees}

    \begin{tabular}{l}
        \textbf{Acyclic} subset of edges containing all nodes \textbf{with minimum weight}\\
        \tabitem MST != Shortest paths\\
        \tabitem Assume edge weight distinct\\\\
        \textbf{3 Basic Properties:}\\
    \end{tabular}

    \noindent\begin{tabular}{ll}
        \textbf{1.} & \textbf{No cycles}\\\\
        \textbf{2.} & If you cut an MST, \textbf{2 pieces are MSTs}\\\\
        \textbf{3.1.} & Cycle Property: For every cycle in the graph, \textbf{MAXIMUM weight edge is NOT in the MST}\\
        \textbf{3.2.} & False Cycle Property: Minimum weight edge in a cycle may or may not be in a MST\\\\
        \textbf{4.} & Cut Property: For every partition of nodes, \textbf{MINIMUM weight edge IS in the MST}\\
        & - Implies \textbf{for every vertex, minimum outgoing edge IS in the MST}\\\\\\
    \end{tabular}   

    \subsection{Generic MST algorithm}


    \noindent\begin{tabular}{ll}
        \textbf{Red Rule} & If C is a cycle with no red edges, color C's max-weight edge red\\
        \textbf{Blue Rule} & If D is a cut with no blue edges, color D's min-weight edge blue\\
    \end{tabular}

    \begin{verbatim}
        // Greedy Algorithm
        Repeat: 
            Apply red or blue rule to an arbitrary edge
            until no more edges can be coloured

        // On termination, (all) blue edges are an MST
        // Every cycle has a red edge, no blue cycles
        // Every edge is coloured
    \end{verbatim}

    \pagebreak

    \subsection{Prim's Algorithm}

    \begin{tabular}{l}
        S  = set of nodes connected by blue edges\\
        Initially: $S = {A}$\\
        Repeat: Identify Cut {S, V-S}, find minimum weight edge of cut, add new node to S\\
        \textbf{Use of PQ to find lightest edge on cut}\\\\
        Each edge added is lightest on some cut.\\
        $\therefore$ By blue rule, each edge added to S is in the MST\\\\
        \textbf{Assuming use of Binary Heap, running time = } $\bm{O(E log V)}$\\
        \tabitem $\because$ Each vertex added/removed to/from PQ: O(V log V)\\
        \tabitem Each edge: one decreaseKey, O(E log V), and E is at most $V^{2}$\\
    \end{tabular}

    \begin{verbatim}
        // Initialize priority queue
        PriorityQueue pq = new PriorityQueue();
        for (Node v : G.V()) pq.insert(v, INFTY);
        pq.decreaseKey(start, 0);

        // Initialize set S
        HashSet<Node> S = new HashSet<Node>();
        S.put(start);

        // Initialize parent hash table
        HashMap<Node,Node> parent = new HashMap<Node,Node>();
        parent.put(start, null);

        while (!pq.isEmpty()):
            Node v = pq.deleteMin();                    // Pop node
            S.put(v);                                   // Add node to MST
            for each (Edge e : v.edgeList()):           // Iterate through its edges
                Node w = e.otherNode(v);
                if (!S.get(w)):
                    // Assume decreaseKey here does nothing if newWeight > prevWeight
                    pq.decreaseKey(w, e.getWeight());   
                    parent.put(w, v);                   // Keep parent to check the edge
    \end{verbatim}

    \pagebreak

    \subsection{Kruskal's Algorithm}

    \begin{tabular}{l}
        Sort all edges by weight\\
        Consider edges in ascending order:\\
        \tabitem If both endpoints are in blue tree, colour edge red, heaviest edge in cycle\\
        \tabitem Else, colour edge blue\\
        \textbf{Use of Union-find DS} (Connect two nodes if in same blue=tree)\\\\
        Each added edge crosses a cut. Since sorted, edge is lightest across the cut\\
        All other lighter cuts have already been considered\\\\
        \textbf{Running time = } $\bm{O(E log V)}$\\
        \tabitem $\because$ Sorting: O(E log E) = O(E log V) since E is at most $V^{2}$\\
        \tabitem Union Find operations are O(logV) or O($\alpha$(n)) for E edges
    \end{tabular}

    \begin{verbatim}
        // Sort edges and initialize
        Edge[] sortedEdges = sort(G.E());
        ArrayList<Edge> mstEdges = new ArrayList<Edge>();
        UnionFind uf = new UnionFind(G.V());

        // Iterate through all the edges, in order
        for (int i=0; i<sortedEdges.length; i++):
            Edge e = sortedEdges[i];                // get edge
            Node v = e.one();                       // get node endpoints
            Node w = e.two();

            if (!uf.find(v,w)):                     // Not in the same tree?
                mstEdges.add(e);                    // save edge
                uf.union(v,w);                      // combine trees
    \end{verbatim}

    \pagebreak

    \subsection{Boruvka's Algorithm}

    \begin{tabular}{l}
        For each node in the graph, create connected component, each node stores component identifier (O(V))\\\\
        Repeat Boruvka Step: \textbf{O(V+E)} \\
        1. For each connected component, \textbf{search and add minimum-weight outgoing edge}\\
        \tabitem DFS or BFS (O(V+E)), check if edge connects two components, remember minimum cost edge of component.\\
        2. \textbf{Merge selected components}\\
        \tabitem Compute and update new component ids (O(V)), mark added edges\\\\
        \textbf{For k connected components, at least k/2 edges added: }\\
        \tabitem At least k/2 components merged\\
        \tabitem $\therefore$ \textbf{At most k/2 connected components remain}\\
        \tabitem $\therefore$ \textbf{At most O(log V) Boruvka steps}\\\\

        From the above, logV steps take O(V+E) each.\\
        \textbf{Running time} $\bm{= O((E+V)logV) = O(ElogV)}$\\\\

        \textbf{Advantage: }Each connected component can perform a Boruvka step mostly independently, except merging\\
    \end{tabular}

    \pagebreak

    \subsection{MST Variations}

    \subsubsection{Edges with same weight}

    \begin{tabular}{l}
        DFS/BFS, Edge in spanning tree = V-1 = Edges in MST.\\
        $\therefore$ \textbf{Any spanning tree found with BFS/DFS is an MST}
    \end{tabular}

    \subsubsection{All edges have a known range}
    \begin{tabular}{l}
        \textbf{Kruskal Variation:} O($\alpha$E) time\\
        Counting Sort using an array of size(range)\\
        \tabitem Put edges in array of linked lists O(E)\\
        \tabitem Iterate over all edges in ascending order O(E)\\
        \tabitem For each edge: Check whether to add an edge O($\alpha$) and union two components if needed O($\alpha$)\\\\
        \textbf{Prim Variation: }O(V+E) = O(E) time\\
        Use an array of size 10 as PQ, A[j] holds linked lists of nodes of weight j\\
        Insertion/removal of nodes: O(V)\\
        decreaseKey: Move node to new linked list in O(E)\\

    \end{tabular}

    \subsubsection{Directed Acyclic Graphs}

    \begin{tabular}{l}
        Much harder problem to solve. For special case: DAG with \textbf{Single possible route}:\\
        \tabitem For every node except the root, add min-weight incoming edge.\\
        \tabitem $\because$ Every node has at least one incoming edge in the MST, each edge chosen only once, V-1 edges\\
        \tabitem O(E) time\\
    \end{tabular}

    \subsubsection{Maximum Spanning Tree, adding k to edge weights}

    \begin{tabular}{l}
        MST algorithms only care about relative edge weights; nothing changes if multiply edges by k, where $k > 0$, or add/subtract\\
        MST with negative weights? Doesn't matter, only relative edge weights matter.\\\\
        \textbf{Maximum Spanning Tree: Negate edge weights, run MST algo}, or run Kruskal's/Prim's in reverse\\
    \end{tabular}

    \subsubsection{Steiner Tree problem}

    \begin{tabular}{l}
        Find MST of a subset of the vertices (required nodes), but can use other (Steiner) nodes.\\
        \textbf{NP-Hard problem}: 2-approx algorithm exits, $T < 2 * Optimal(G)$\\
        1. For every pair of required vertices, calculate shortest path: Djisktra V times, or any All-Pairs-Shortest-Paths\\
        2. Construct new graph G on required nodes using edge weights found.\\
        3. Run MST algo on G; MST found.\\
        4. Map edges back to original graph.\\
    \end{tabular}


    \pagebreak

    \section{Dynamic Programming}

    \begin{tabular}{l}
        \textbf{Optimal Sub-structure: }Optimal soln can be constructed from optimal solns to smaller sub-problems\\
        \textbf{Overlapping Subproblems}\\
        \tabitem Use of \textbf{memoization} and a 'table' to remember the data\\
    \end{tabular}

    \subsection{Longest increasing subsequence}
    \begin{tabular}{l}
        For Array A[1..n], find longest increasing (not necessarily consecutive) sequence of numbers\\
        Define sub-problems: S[i] = LIS(A[i..n]) starting at A[i]\\
        Solve using subproblems: $S[n] = 0$ and $S[i] = (max_{(i,j) \in E} S[j]) + 1$ (Maximum of traversed nodes)\\
    \end{tabular}

    \begin{verbatim}
        LIS(V): // Assume graph is already topo-sorted
            int[] S = new int[V.length];                    // Create memo array
            for (i=0; i<V.length; i++) S[i] = 0;            // Initialize array to zero
            S[n-1] = 1;                                     // Base case: node V[n-1]
            for (int v = A.length-2; v>=0; v--):
                int max = 0;                                // Find maximum S for any outgoing edge
                for (Node w : v.nbrList()):                 // Examine each possible outgoing edge
                    if (S[w] > max) max = S[w];             // Check S[w], which we already calculated earlier.
            S[v] = max + 1;                                 // Calculate S[v] from max of outgoing edges.
    \end{verbatim}

    
    \begin{tabular}{l}
        Alternate, similar definition: sub-problem being S[i] = LIS(A[1..i]) \textbf{ending} at A[i]\\
        Both definitions: $O(n^{2})$ total time (n subproblems, subproblem i takes O(i))\\
        O(nlogn) using Binary Search to solve faster\\
    \end{tabular}

    \pagebreak

    \subsection{(Lazy) Prize Collecting}

    \begin{tabular}{l}
        Graph with negative and positive edge weights; Find path to get as high amount as possible.\\
        Limit k: What is the maximum prize collected by crossing at MOST k edges in the graph?\\\\
        \textbf{1. Define Sub-problem:} \\
        \tabitem P[v, k] = maximum prize that you can collect starting at v and taking EXACTLY k steps.\\
        \tabitem P[v, 0] = 0 \\
        \textbf{2. Use sub-problems to solve P[v,k]:}\\
        \tabitem P[v, k] = MAX { P[w1, k-1] + w(v, w1), P[w2, k-1] + w(v, w2), … }\\
        \tabitem At every P[v, k] subproblem, save result in a table of v by k.\\
    \end{tabular}

    \begin{verbatim}
        int LazyPrizeCollecting(V, E, kMax) {
            int[][] P = new int[V.length][kMax+1];                     // create memo table P
            // initialize P to zero
            for (int i=0; i<V.length; i++) for (int j=0; j<kMax+1; j++) P[i][j] = 0;    
        
            for (int k=1; k<kMax+1; k++) {                             // Solve for every value of k
                for (int v = 0; v<V.length; v++) {                     // For every node…
                    int max = -INFTY;
                    for (int w : V[v].nbrList()) {                     // …find max prize in next step
                        if (P[w,k-1] + E[v,w] > max)
                        max = P[w,k-1] + E[v,w];
                    }
                    P[v, k] = max;
                }
            }
            return maxEntry(P); // returns largest entry in P
        }
    \end{verbatim}


    \begin{tabular}{l}
        \tabitem Looks like a O(kVE) problem, but loose bound; don't have to go through all edges.\\
        \tabitem $O(kV^{2})$ if you take kV subproblems, each costing $|v.nbrList|$ which is maximum V.\\
        \tabitem $O(kE)$ from table: k rows, O(E) cost to solve each row! (Since you look at each edge once per row)\\
    \end{tabular}

    \pagebreak

    \subsection{Vertex Cover}

    \begin{tabular}{l}
        Given undirected, unweighted graph G, find set of nodes C where every edge is adjacent to at least one node in C.\\
        \tabitem NP-complete, easy \textbf{2-approximation}\\\\
        \textbf{Special Case:} Given an undirected, unweighted \textbf{tree } and its root r, find size of vertex cover of this tree\\
        \textbf{Subproblem?} For subtree rooted at v,\\
        1. S[v, 0]: Size of vertex cover in subtree rooted at v, \textbf{if v is NOT covered}\\
        \tabitem S[v, 0] = S[w1, 1] + S[w2, 1] + S[w3, 1] + …\\
        \tabitem Since children HAVE to be already covered\\
        2. S[v, 1]: Size of vertex cover in subtree rooted at v, \textbf{if v IS covered}\\
        \tabitem S[v, 1] = 1 + min(S[w1, 0], S[w1, 1]) + min(S[w2, 0], S[w2, 1])\\
        \tabitem Since doesnt matter whether children are covered or not\\
    \end{tabular}

    \begin{verbatim}
        int treeVertexCover(V){//Assume tree is ordered from root-to-leaf
            int[][] S = new int[V.length][2];   // create memo table S

            for (int v=V.length-1; v>=0; v--){  //From the leaf to the root
                if (v.childList().size()==0) {  // If v is a leaf…
                    S[v][0] = 0;
                    S[v][1] = 1;
                } else{ // Calculate S from v’s children.
                    int S[v][0] = 0;
                    int S[v][1] = 1;
                    for (int w : V[v].childList()) {
                        S[v][0] += S[w][1];
                        S[v][1] += Math.min(S[w][0], S[w][1]);
                    }
                }
            }
            return Math.min(S[0][0], S[0][1]); // returns min at root
        }
    \end{verbatim}
    
    \begin{tabular}{l}
        \tabitem Looks like O$(V^{2})$, since 2V sub-problems, O(V) time per\\
        \tabitem \textbf{O(V)} time to solve all subproblems, since each of the (V-1) edges is only explored once\\
    \end{tabular}

    \pagebreak

    \subsection{All-Pairs Shortest Paths}

    \begin{tabular}{l}
        Given directed, connected weighted graph G, answer queries: Preprocess graph, and answer min-dist(v, w)\\\\
        Simple Soln: Dijkstra on first query from source v, save all min-dists from v to all other nodes\\
        \tabitem 0 Preprocessing, O(V E logV) to respond to q queries (Max need run Dijkstra V times)\\
        \tabitem If run APSP during preprocessing, responding to q queries: O(q)\\
        ]tabitem In a sparse graph, $O(V^{2}logV)$\\
        \tabitem in an unweighted graph, use BFS for O(V(E+V)): $O(V^{3})$ for dense graphs,  $O(V^{2})$ for sparse\\
    \end{tabular}

    \subsubsection{Floyd-Warshall}
    
    \begin{tabular}{l}
        \textbf{Optimal Substructure}: If P is shortest path from u to v to w, then P contains shortest paths from u to v and v to w.\\
        \textbf{Subproblem:} S[v,w,P] = Shortest path from v to w that only uses intermediate nodes in set P\\
        \tabitem Base case: S[v, w, $\emptyset$] = E[v,w] (Direct edge from v to w)\\
        S[v,w,P8] = min( S[v, w, P7], \textbf{S[v, 8, P7] + S[8, w, P7]} ), where set P8 adds node 8 to set P7\\
    \end{tabular}

    \begin{verbatim}
        int[][] APSP(E){ // Adjacency matrix E
            int[][] S = new int[V.length][V.length]; //create memo table S

            // Initialize every pair of nodes
            for (int v=0; v<V.length; v++)
                for (int w=0; w<V.length; w++)
                    S[v][w] = E[v][w];

            // For sets P0, P1, P2, P3, …, for every pair (v,w)
            for (int k=0; k<V.length; k++)
                for (int v=0; v<V.length; v++)
                    for (int w=0; w<V.length; w++)
                        S[v][w] = min(S[v][w], S[v][k]+S[k][w]);
            return S;
        }
    \end{verbatim}

    \begin{tabular}{l}
        Running time: $O(V^{3}$\\
    \end{tabular}


    \pagebreak

    \section{Data Structures Summary}

    \subsection{Trees}

    \noindent\begin{tabular}{|l|l|l|l|p{5cm}|}
        \toprule
        \textbf{Name} & \textbf{Search} & \textbf{Insert} & \textbf{Delete} & \textbf{Remarks}\\
        \midrule
        \midrule
        \textbf{BST} & $O(height)$ & $O(height)$ & $O(height)$ & $h < 2log(n)$\\
        \midrule
        \textbf{AVL} & $O(logn)$ & $O(logn) +$ 2 rotations & $O(logn)$ + logn rotations & If v is left-heavy,\\
        &&&&- v.left is balanced/left-heavy: right-rotate(v)\\
        &&&&- v.left is right-heavy: left-rotate(v.left), right-rotate(v)\\
        \midrule
        \textbf{Trie} & $O(length)$ & $O(length)$& $O(length)$ &\\
        \midrule
        \textbf{(a,b)-trees} & $O(logn)$ & $O(logn)$ & $O(logn)$ & Insert: split\\
        \textbf{B-trees} &&&& Delete: Merge, or Share (merge + split)\\
        \bottomrule
    \end{tabular}

    \subsection{Augmented Trees}

    Assuming augmented from AVL, search(), insert() and delete() are $O(logn)$\\

    \noindent\begin{tabular}{|l|l|}
        \toprule
        \textbf{Order Statistics} & \textbf{Find order/rank of nodes}\\
        & \tabitem Store size of sub-tree in every node\\
        & \tabitem During insertion, maintain weight during rotation\\
        \midrule
        \textbf{Interval Tree} & Nodes sorted by left endpoint\\
        & Nodes contain max endpoint in tree rooted at node\\
        \midrule 
        \textbf{Orthogonal Range Searching} & \textbf{Find everything within certain range}\\
        (kd-trees) & \tabitem Points stored in leaves\\
        & \tabitem internal node stores max(node.left)\\
        & \tabitem kd-trees: Alterate splitting between dimensions:\\
        & \tabitem Query: $O(\sqrt{n} + k)$, Space: $O(n)$, Build: $O(nlogn)$\\
        \midrule
        \textbf{Range Tree} & \textbf{Build x-tree using only x-coords, x-node contains y-tree (etc)}\\
        & \tabitem Query: $O(log^{2}n + k)$, Space: $O(nlogn)$, Build: $O(nlogn)$\\
        \bottomrule 
    \end{tabular}

    \subsection{Hashing}

    Assuming m is number of buckets, n is number of keys, h is cost of hash function,\\

    \noindent\begin{tabular}{|l|l|l|l|p{6.5cm}|}
        \toprule
        \textbf{Name} & \textbf{Search} & \textbf{Insert} & \textbf{Space} & \textbf{Remarks}\\
        \midrule
        \midrule
        \textbf{Chaining} & $O(h + n/m)$ & $O(h + 1)$ & $O(m+n)$ & \tabitem Simple Uniform Hashing Assumption\\
        & $= O(1)$ (Expected) &&&\tabitem load $= n/m$ \\
        & $= O(n)$ (Worst-case) &&&\\
        \midrule
        \textbf{Open-Addressing} &$O(1)$&$1/(1-load)$& $O(n)$ &\tabitem Uniform Hashing Assumption\\
        &&&&\tabitem Redefine hash function: Linear Probing or otherwise\\
        &&&&\tabitem Double-hashing:\\
        &&&& $h(k, i) = [f(k) + ig(k)]$ $mod$ m\\
        &&&&\tabitem Tombstone value for deleted items\\
        &&&&\tabitem Performance degrades as load $= n/m$ tends to 1\\
        \bottomrule
    \end{tabular}


    

    

\end{document}


