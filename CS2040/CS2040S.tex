\documentclass{article}

% Formatting purposes
\usepackage[margin=1in]{geometry}

% Hyperlink functionality
% For \url
\usepackage{hyperref}

% Graphics Functionality
% For \includegraphics, \graphicspath{} and \begin{figure}
\usepackage{graphicx, caption, subcaption}

% Table functionality
\usepackage{multirow, multicol}
\usepackage{attrib}

\usepackage{bm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}% http://ctan.org/pkg/booktabs
\newcommand{\tabitem}{~~\llap{\textbullet}~~}

\title{CS2040 Notes}
\author{Koh Jia Xian}

\begin{document}
    \maketitle
    \tableofcontents

    \pagebreak

    \section{Definitions}

    \subsection{Time and Space Complexity}

    \begin{itemize}
        \item Space Complexity = \textbf{Total} space ever allocated
        \item Amortized cost $T(n)$ if $\forall k \in \mathbb{Z}$, cost of operation is $\leqq kT(n)$
    \end{itemize}

    \subsubsection{Big O}

    $T(n) = O(f(n))$ if:
    \begin{enumerate}
        \item There exists a constant $c > 0$
        \item and a constant $n_{0} > 0$
    \end{enumerate}
    such that for all $n > n_{0}$, \\\\$\bm{T(n) \leq c f(n)}$\\\\
    \emph{ie) An upper bound above a certain size n; Always try to get the tightest bound}

    \subsubsection{Big Omega}

    $T(n) = \Omega(f(n))$ if:
    \begin{enumerate}
        \item There exists a constant $c > 0$
        \item and a constant $n_{0} > 0$
    \end{enumerate}
    such that for all $n > n_{0}$, \\\\$\bm{T(n) \geq c f(n)}$\\\\
    \emph{ie) A lower bound above a certain size n}

    \subsection{Pre and Post-conditions}

    \begin{table}[htbp]
        \begin{tabular}{ll}
            \textbf{Precondition} & Fact that is true when the function begins\\
            \textbf{Postcondition} & Fact that is true when the function ends\\
        \end{tabular}
    \end{table}

    \subsection{Invariants}
        \begin{tabular}{ll}
            \textbf{Invariants} & Relationship between variables that is \textbf{always true}.\\
            \textbf{Loop Invariants} & Relationship between variables that is true at the beginning (or end) of each iteration of a loop.\\
        \end{tabular}


    \subsection{Stability and In-Place sorting}
    When 2 of the same keys are sorted:
    \begin{itemize}
        \item If its value becomes out of order, \textbf{Unstable}
        \item \textbf{Stability: Preserving order of repeated elements}
    \end{itemize}

    General Rule-of-Thumb, if got swap here-swap there (ie \textbf{NOT IN-PLACE}), it is unstable

    \subsection{Probability and Expected Value}

    
    \begin{itemize}
        \item $E[X] = e_{1}p_{1} + e_{2}p_{2} + ... + e_{k}p_{k}$
        \item $E(A+B) = E(A) + E(B)$
    \end{itemize}


    \subsection{Trees}

    \begin{tabular}{ll}
        \textbf{Successor} & Next largest value in the tree.\\
        \textbf{Height} & Number of edges on longest path from root to leaf.\\
        & \tabitem $h(v) = 0$ if v is a leaf\\
        & \tabitem $h(v) = max(h(v.left), h(v.right)) + 1$\\
    \end{tabular}

    

    \pagebreak

    \section{Common Time Complexities}


    \begin{tabular}{|l|c|l|}
        \toprule
        \textbf{Recurrence} & \textbf{Complexity} & \textbf{Remarks}\\
        \toprule
        $T(n) = 2T(n/2) + O(n)$ & $O(nlogn)$ & Height of logn, n each 'level'\\
        \midrule
        $T(n) = T(n / 2) + O(1)$ & $O(logn)$ & Height of logn, 1 each 'level'\\
        \midrule
        $T(n) = 2T(n / 2) + O(1)$ & $O(n)$ & 1, 2, 4, ... n: Sum of GP\\
        \midrule
        $T(n) = T(n / 2) + O(n)$ & $O(n)$ & n, n/2, n/4 ... 1: Sum of GP\\ 
        \bottomrule
    \end{tabular}

    \subsection{AP GP Sums}

    \begin{tabular}{l}
        \toprule
        For AP, $S_{n} = \frac{1}{2}n(a_{1} + a_{n})$\\
        \tabitem If AP is 1, 2,...n, $S_{n} = \frac{n^{2} + n}{2} = O(n^{2})$\\
        \midrule
        For GP, $S_{n} = \frac{a(r^{n}-1)}{r-1} = \frac{a(1 - r^{n})}{1- r}$\\
        \tabitem Sum to $\infty$ $S_{\infty} = \frac{a}{1-r}$\\
        \tabitem If GP is 1, 2, 4...n, where a = n, $r = 1/2$, $S_{n} = \frac{a}{1-r} = \frac{n}{1-0.5} = O(n)$\\
    \end{tabular}

    \pagebreak

    \section{Binary Search}
    For a \textbf{sorted }array, take middle, compare to key: search LHS or RHS of mid.

    \begin{verbatim}
    int search(A, key, n)
        begin = 0
        end = n-1
        while begin < end do:
            mid = begin + (end-begin)/2;
            if key <= A[mid] then
                end = mid
            else begin = mid+1
        return (A[begin]==key) ? begin : -1    
    \end{verbatim}


    \begin{tabular}{ll}
        \toprule
        \textbf{Functionality} & \tabitem If element not in array, return index\\
        & \tabitem If element not in array, return -1\\
        \midrule
        \textbf{Precondition} & \tabitem Array is of size n\\
        & \tabitem Array is sorted\\
        \midrule
        \textbf{Postcondition} & If element is in the array: A[begin] = key\\
        \midrule
        \textbf{Invariant (Correctness)} & $A[begin] \leq key \leq A[end]$ \\
        & \tabitem \emph{The key is in the range of the Array}\\
        \midrule
        \textbf{Invariant (Speed)} & $(end-begin) \leq n/2^{k}$ in iteration k \\
        \bottomrule
    \end{tabular}

    \bigskip
    \textbf{Not just for searching Arrays:}
    \begin{enumerate}
        \item \begin{itemize}
            \item Assuming a complicated function,
            \item Assume function is always increasing: $complicatedFunction(i) < complicatedFunction(i+1)$
            \item $\therefore$ Find minimum value j such that $complicatedFunction(j) > 100$
        \end{itemize}

        \item Peak Finding (1 or 2 Dimensions)
        \item QuickSelect
    \end{enumerate}

    \pagebreak

    \subsection{Peak Finding}

    Want to find an index i such that $\bm{arr[i] \geq arr[i-1]}$ \&  $\bm{arr[i] \leq arr[i+1]}$

    \begin{verbatim}
    FindPeak(A, n)
        //Recurse on right
        if A[n/2+1] > A[n/2] then
            FindPeak(A[n/2+1..n], n/2)

        //Recurse on left
        else if A[n/2–1] > A[n/2] then
            FindPeak(A[1..n/2‐1], n/2)

        else A[n/2] is a peak; return n/2

    \end{verbatim}
    
    \begin{tabular}{ll}
        \toprule
        \textbf{Functionality} & On an unsorted array, find A peak: \\
        & local minimum or maximum (not a specific key)\\
        \midrule
        \textbf{Invariants (Correctness)} & \tabitem There exists a peak in the range $[begin, end]$\\
        & Every peak in $[begin, end]$ is a peak in $[1, n]$.\\
        \midrule
        \textbf{Running Time} & $T(n) = T(n/2) + \theta(1)$\\
        & Recurse for $log2(n)$ times\\
        & $\therefore O(logn)$\\
        \bottomrule
    \end{tabular}

    \subsection{Steep Peaks}

    Want to find a peak such that its left and right side are \textbf{strictly lower than it}.\\\\

    \begin{tabular}{ll}
        \toprule
        \textbf{Functionality} & On an unsorted array, find A peak: local minimum or maximum (not a specific key)\\
        & \textbf{If both sides are the same as mid, recurse both sides}\\
        \midrule
        \textbf{Running Time} & $T(n) = $\textbf{2}$T(n/2) + \theta(1)$\\
        & $ = 16T(n/16) + 8  + 4 + 2 + 1$\\
        & $...$\\
        & $ = nT(1) + n/2 + n/4 + ... + 1$ \\
        & $\bm{ = O(n)}$ \textbf{Sum of Geometric Progression}\\
        \bottomrule
    \end{tabular}

    \pagebreak

    \subsection{QuickSelect}

    Find kth smallest element\\

    \noindent Makes use of QuickSort's \hyperref[partition]{partition} to ensure that the kth smallest element 
    is before or after the randomly selected pivot

    \begin{verbatim}
    Select(A[1..n], n, k)
        if (n == 1) then return A[1];
        else Choose random pivot index pIndex.
            p = partition(A[1..n], n, pIndex)
            if (k == p) then return A[p];
            else if (k < p) then
                return Select(A[1..p–1], k)
            else if (k > p) then
                return Select(A[p+1], k – p)
    \end{verbatim}

    \noindent Recurrence: $T(n) = T(n/2) + O(n)$\\

    \noindent Time Complexity: $\bm{O(n)}$ (Sum of G.P.)

    \subsubsection{Paranoid Select}

    Repeatedly partition until at least n/10 in each half of partition

    $E[T(n)] \leq E[T(9n/10)] + E[num of partitions](n)$

    $\leq E[T(9n/10)] + 2n$

    $\leq O(n)$

    \pagebreak

    \section{Sorting}

    \subsection{Bubble Sort}

    Iteratively swap largest values to the top.

    \begin{verbatim}
    BubbleSort(A, n)
        repeat (until no swaps) :
            for j <- 1 to n-1
                if A[j] > A[j+1] then swap(A[j], A[j+1])

    \end{verbatim}
    
    \begin{tabular}{ll}
        \toprule
        \textbf{Loop Invariant} & At the end of iteration j, the biggest j items are correctly sorted \\
        & in the \textbf{final j positions} of the array.\\
        \midrule
        \textbf{Invariant (Correctnness)} & Sorted after n iterations\\
        \midrule
        \textbf{Running Time} & \\
        \tabitem Best Case & $O(n)$ [Already Sorted]\\
        \tabitem Average Case & $O(n^{2})$\\
        \tabitem Worst Case & $O(n^{2})$ [n iterations]\\
        \midrule
        \textbf{Space Consumption} & $O(1)$\\
        \midrule
        \textbf{Stability} & \textbf{Stable}, only swap elements that are different\\
        \bottomrule
    \end{tabular}

    \bigskip

    \subsection{Selection Sort}

    Find minimum element and swap it directly with the front.

    \begin{verbatim}
    SelectionSort(A, n)
        for j <- 1 to n-1:
            find minimum element A[j] in A[j..n]
            swap(A[j], A[k])
    \end{verbatim}
    
    \begin{tabular}{ll}
        \toprule
        \textbf{Loop Invariant} & At the end of iteration j: the smallest j items are correctly sorted \\
        & in the \textbf{first j positions} of the array.\\
        \midrule
        \textbf{Running Time} & $n + (n-1) + (n-2) + ... + 1$\\
        & $ = \frac{n(n-1)}{2}$ (Sum of A.P.)\\
        & $ = O(n^{2})$\\
        \tabitem Best Case & $O(n^{2})$ [If already Sorted, will swap anyway]\\
        \tabitem Average Case & $O(n^{2})$\\
        \tabitem Worst Case & $O(n^{2})$ [n swaps]\\
        \midrule
        \textbf{Space Consumption} & $O(1)$\\
        \midrule
        \textbf{Stability} & \textbf{Unstable}, swap changes order\\
        \bottomrule
    \end{tabular}

    \pagebreak

    \subsection{Insertion Sort}

    Iteratively swaps the current element into its rightful place in the sorted left side of the array.

    \begin{verbatim}
    InsertionSort(A, n)
        for j <- 2 to n
            key <- A[j]
            i <- j-1
            while (i > 0) and (A[i] >key)
                A[i+1] <- A[i]
                i <- i-1
            A[i+1] <- key
    \end{verbatim}
    
    \begin{tabular}{ll}
        \toprule
        \textbf{Loop Invariant} & At the end of iteration j: the \textbf{first j items} in the array \\
        &  are in sorted order.\\
        \midrule
        \textbf{Running Time} & $1 + 2 + 3 + ... + n$\\
        & $ = \frac{n(n-1)}{2}$ (Sum of A.P.)\\
        & $ = O(n^{2})$\\
        \tabitem Best Case & $O(n)$ [Already Sorted]\\
        \tabitem Average Case & $O(n^{2})$\\
        \tabitem Worst Case & $O(n^{2})$ [Inverse Sorted]\\
        \midrule
        \textbf{Space Consumption} & $O(1)$\\
        \midrule
        \textbf{Stability} & \textbf{Stable}, swap doesn't change order, \\
        & as long as implemented properly (\textbf{A[i] $>$ key})\\
        \bottomrule
    \end{tabular}

    \bigskip

    Insertion Sort can be fast(er than MergeSort!) if \textbf{List is mostly sorted}


    \subsection{MergeSort}

    \textbf{Divide-and-Conquer}, sort two halves, merge two sorted halves

    \begin{verbatim}
    MergeSort(A, n)
        if (n=1) then return;                                               //O(1)
        else:           
            X <- MergeSort(A[1..n/2], n/2);                                 //T(n/2)
            Y <- MergeSort(A[n/2+1, n], n/2);                               //T(n/2)
            return Merge (X,Y, n/2); //2 sorted halves combined together    //O(n)
    \end{verbatim}
    
    \begin{tabular}{ll}
        \toprule
        \textbf{Running Time} & \\
        Running Time of Merge & Given A and B of sizes n/2, $\bm{O(n)}$ to move each element back into list\\
        & $\therefore T(n) = O(1)$ (if $n = 1$)\\
        & $= 2T(n/2) + cn$ (if $n > 1$)\\
        & $\therefore$ Height of recursion tree $h = logn$, every level $cn$ operations\\
        & $\therefore T(n) = cnlogn$, $\bm{O(n) = nlogn}$\\
        \tabitem Best Case & $O(nlogn)$\\
        \tabitem Average Case & $O(nlogn)$\\
        \tabitem Worst Case & $O(nlogn)$\\
        \midrule
        \textbf{Space Consumption} & $\bm{O(n)}$ [Using 1 temporary array, Switch the order of A and B at
        every recursive call.]\\
        \midrule
        \textbf{Stability} & \textbf{Stable}\\
        \bottomrule
    \end{tabular}

    \bigskip

    MergeSort can be slower for \textbf{Smaller number of items to sort}

    \pagebreak

    \subsection{QuickSort}

    Separate larger and smaller than a chosen \textbf{pivot} (Partitioning), recursively sort both sub-arrays.
    
    \label{partition}
    
    \begin{verbatim}
     QuickSort(A[1..n], n)
        if (n==1) then return;
        else
            Choose pivot index pIndex   //How?
            p = partition(A[1..n], n, pIndex)
            x = QuickSort(A[1..p-1], p-1)
            y = QuickSort(A[p+1..n], n-p)

    //Returns the index of the pivot
    partition(A[1..n], n, pIndex)       // Assume no duplicates, n>1
        pivot = A[pIndex];              // pIndex is the index of pivot
        swap(A[1], A[pIndex]);          // store pivot in A[1]
        low = 2;                        // start after pivot in A[1]
        high = n+1;                     // Define: A[n+1] = Infinity
        while (low < high)
            while (A[low] < pivot) and (low < high) do low++;
            while (A[high] > pivot) and (low < high) do high– – ;
            if (low < high) then swap(A[low], A[high]);
        swap(A[1], A[low–1]);
        return low–1;
    \end{verbatim}
    
    \begin{tabular}{ll}
        \toprule
        \textbf{Invariants} & \tabitem For every $i \geq high$ : $A[i] > pivot$\\
        & \tabitem For every $1 < j < low$ : $A[j] < pivot$\\
        \textbf{Running Time} & \\
        Running Time of Partition & $O(n)$\\
        \tabitem Best Case & $O(nlogn)$\\
        \tabitem Average Case & $O(nlogn)$\\
        \tabitem Worst Case & $O(n^{2})$ [eg All elements duplicates]\\
        \midrule
        \textbf{Space Consumption} & $O(1)$\\
        & \emph{Extra Memory allows QuickSort to be stable}\\
        \midrule
        \textbf{Stability} & \textbf{Unstable}\\
        \bottomrule
    \end{tabular}


    

    \subsection{QuickSort Optimisations}

    \subsubsection{Base Case?}
    \begin{itemize}
        \item Unoptimized: Recurse to single-element arrays
        \item Switch to Insertion Sort for small arrays (Relies on fact that InsertionSort is fast for small arrays)
        \item Halt Recursion early, leaving small arrays unsorted. Then perform InsertionSort on entire array
    \end{itemize}
    
    \pagebreak

    \subsubsection{3-Way Partitioning}

    Deal with duplicates in arrays

    \begin{tabular}{ll}
        \textbf{Option 1} & \textbf{2-pass Partitioning}\\
        & 1. Regular Partition\\
        & 2. Pack Duplicates (of pivot) together\\
        \textbf{Option 2} & \textbf{1-pass Partitioning}\\
        & \tabitem Standard Solution\\
        & \tabitem Mantain Four Regions of Array (See Fig \ref{1pass})\\
    \end{tabular}

    \begin{figure}[htbp]
        \begin{center}
        \includegraphics[width=4in,keepaspectratio]{3-way.JPG}
        \caption{1-pass Partitioning}
        \label{1pass}
        \end{center}
    \end{figure}

    \begin{tabular}{ll}
        \textbf{ If }$bm{A[current] < pivot}$ & low++\\
        & Swap $A[current]$, $A[low]$\\
        & current++\\
        \textbf{ If} $bm{A[current] == pivot}$ & current++\\
        \textbf{ If} $bm{A[current] > pivot}$ & Swap $A[current]$, $A[high]$ \\
        & high--\\
    \end{tabular}

    \subsubsection{Choice of Pivot}
    In the worst case(s),
    
    \noindent\begin{tabular}{ll}
        \textbf{First Element} & A[1]\\
        \textbf{Last Element} & A[n]\\
        \textbf{Middle Element} & A[n/2]\\
        \textbf{Median of first, last and middle} & Median of the above 3\\
    \end{tabular}
    
    \noindent are equally bad, if \textbf{n executions of partition, sorting 1 element each: }\\
    
    $T(n) = T(n-1) + T(1) + n$
    
    (From Quicksort of n-1 elements + QuickSort on 1 element + Cost of partition on n elements)

    $\bm{\therefore O(n^{2})}$ \textbf{time}.\\

    \noindent\textbf{If can choose Median: Good Performance }$\bm{O(nlogn)}$\\
    
    \noindent\textbf{If could split array (1:10) : (9:10): Good Performance }$\bm{O(nlogn)}$\\\\
    $\therefore$ A pivot is \textbf{good} if divides array into 2 pieces, each of which is size \textbf{at least} $\bm{n/10}$

    \noindent\textbf{Choose pivot at random: PARANOID QUICKSORT}

    Repeat partition until $p > (1/10)n$ and $p < (9/10)n$, 

    Expected number of times to choose a good pivot: $10/8 \approx 2$
    
    $T(n) = T(n-1) + T(1) + 2n$\emph{(Expected no. of iterations to repeat is 2)} 
    
    Hence, worst-case expected time $\bm{= O(nlogn)}$


    \pagebreak

    \section{Sorting Summary}

    \begin{tabular}{|l||l|l|l|l|l|}
        \toprule
        \textbf{Name} & \textbf{Best Case} & \textbf{Average Case} & \textbf{Worst Case} & \textbf{Extra Memory} & \textbf{Stable}\\
        \midrule
        \midrule
        \textbf{Bubble Sort} & $O(n)$ & $O(n^{2})$ & $O(n^{2})$ & $O(1)$ & Yes\\
        % \hline
        \textbf{SelectionSort} & $O(n^{2})$ & $O(n^{2})$ & $O(n^{2})$ & $O(1)$ & No\\
        % \hline
        \textbf{Insertion Sort} & $O(n)$ & $O(n^{2})$ & $O(n^{2})$ & $O(1)$ & Yes\\
        % \hline
        \textbf{Merge Sort} & $O(nlogn)$ & $O(nlogn)$ & $O(nlogn)$ & $O(n)$ & Yes\\
        % \hline
        \textbf{Quick Sort} & $O(nlogn)$ & $O(nlogn)$ & $O(n^{2})$ & $O(1)$ & No\\
        \bottomrule
    \end{tabular}

    \subsection{Remarks}
    \begin{itemize}
        \item BubbleSort vs InsertionSort: InsertionSort faster for almost-sorted arrays
        \item Paranoid Quicksort Worstcase: $O(nlogn)$
        \item \textbf{Any others?}
    \end{itemize}

    \subsection{Invariants}
    \begin{tabular}{|l||l|}
        \toprule
        \textbf{Name} & \textbf{Invariant}\\
        \midrule
        \midrule
        \textbf{Bubble Sort} &  At the end of iteration j, the biggest j items are correctly sorted \\
        & in the \textbf{final j positions} of the array.\\
        \hline
        \textbf{SelectionSort} & At the end of iteration j: the smallest j items are correctly sorted \\
        & in the \textbf{first j positions} of the array.\\
        \hline
        \textbf{Insertion Sort} & At the end of iteration j: the \textbf{first j items} in the array \\
        &  are in sorted order.\\
        \hline
        \textbf{Merge Sort} & idk lmfao probably something about at the end of iteration j of merge\\
        & every $2^{j}$ group of items are in sorted order, where $2^{j} < n$ (???)\\
        & just pulling something out of my ass :)\\ 
        \hline
        \textbf{Quick Sort} & \tabitem For every $i \geq high$ : $A[i] > pivot$\\
        & \tabitem For every $1 < j < low$ : $A[j] < pivot$\\
        \bottomrule
    \end{tabular}


    \begin{tabular}{|l|c|l|}
        \toprule
        \textbf{Recurrence} & \textbf{Complexity} & \textbf{Remarks}\\
        \toprule
        $T(n) = 2T(n/2) + O(n)$ & $O(nlogn)$ & Height of logn, n each 'level'\\
        \midrule
        $T(n) = T(n / 2) + O(1)$ & $O(logn)$ & Height of logn, 1 each 'level'\\
        \midrule
        $T(n) = 2T(n / 2) + O(1)$ & $O(n)$ & 1, 2, 4, ... n: Sum of GP\\
        \midrule
        $T(n) = T(n / 2) + O(n)$ & $O(n)$ & n, n/2, n/4 ... 1: Sum of GP\\ 
        \bottomrule
    \end{tabular}

    \subsection{AP GP Sums}

    \begin{tabular}{l}
        \toprule
        For AP, $S_{n} = \frac{1}{2}n(a_{1} + a_{n})$\\
        \tabitem If AP is 1, 2,...n, $S_{n} = \frac{n^{2} + n}{2} = O(n^{2})$\\
        \midrule
        For GP, $S_{n} = \frac{a(r^{n}-1)}{r-1} = \frac{a(1 - r^{n})}{1- r}$\\
        \tabitem Sum to $\infty$ $S_{\infty} = \frac{a}{1-r}$\\
        \tabitem If GP is 1, 2, 4...n, where a = n, $r = 1/2$, $S_{n} = \frac{a}{1-r} = \frac{n}{1-0.5} = O(n)$\\
    \end{tabular}
    
    \pagebreak

    \section{Trees}

    Data Structure: Implementing a Dictionary, for eg

    \subsection{Binary (Search) Trees}

    \begin{tabular}{l}
        \tabitem Binary Tree is either: 1) Empty, 2) A node pointing to 2 binary trees.\\
        \tabitem Binary Search Trees: \textbf{All in left sub-tree} $<$ key $<$ \textbf{All in right sub-tree}\\
        \tabitem \textbf{Binary Tree} is height balanced if every node in the tree is height-balanced.\\
        \tabitem A height-balanced tree with n nodes has height $h < 2log(n)$, $\therefore O(logn)$.\\
    \end{tabular}
    \bigskip

    \noindent\begin{tabular}{ll}
        \textbf{Time Complexity of search(key) in BST:} & Height of tree\\
        & \tabitem $\bm{O(logn)}$ if balanced\\
        & \tabitem Else, worst-case $\bm{O(n)}$\\
    \end{tabular}

    \subsection{Tree Traversal}

    \begin{tabular}{ll}
        \textbf{In-Order:} & Visit left sub-tree, then SELF, then right sub-tree\\
        \textbf{Pre-Order:} & Visit SELF, then left sub-tree, then right sub-tree\\
        \textbf{Post-Order:} & Visit left sub-tree, then right sub-tree, then SELF\\
        \textbf{Level-Order} & Visit EVERY node at that height, then go lower level\\
        \multicolumn{2}{l}{$\bm{O(n)}$ \textbf{Time Complexity} ($\because$ Visit each node once)}\\
    \end{tabular}

    \subsection{Successor Finding}

    \begin{tabular}{ll}
        \textbf{Basic Strategy: successor(key)} & 1. Search for key\\
        &2. If $(result > key)$, then return result.\\
        &3. If $(result \leq key)$, then search for successor
        of result.\\
    \end{tabular}

    \begin{verbatim}
    //Search for the successor of the current TreeNode
    public TreeNode successor(){
        if (rightTree != null) return rightTree.searchMin();

        TreeNode parent = parentTree;
        TreeNode child = this;
        while ((parent != null) && (child == parent.rightTree))
            child = parent;
            parent = child.parentTree;
        }
        
        return parent;
    }
    \end{verbatim}

    \begin{itemize}
        \item $\bm{O(height)}$ \textbf{Time Complexity}
    \end{itemize}


    \pagebreak

    \subsection{Insertion/Deletion}

    Insertion trivial:
    
    If less than node, node.left == null, insert at left else recurse left. 
    
    If more than node, node.right == null, insert at right, else recurse right.\\\\


    \noindent\begin{tabular}{ll}
        \toprule
        \textbf{3 Cases for delete(v):} & \\
        \textbf{No Children}& Remove v\\
        \hline
        \textbf{1 Child}& Remove v, connect child(v) to parent(v)\\
        \hline
        \textbf{2 Children}& 1. x = successor(v)\\
        & 2. delete(x) (which may cause more calls of delete)\\
        & 3. remove(v)\\
        & 4. connect x to left(v), right(v), parent(v)\\
        \bottomrule
    \end{tabular}\\\\

    \begin{itemize}
        \item \textbf{NOTE: Successor of deleted node has at most 1 child!} (A right node)
        \item $\bm{O(height)}$ \textbf{Time Complexity} (BOTH insertion and deletion)
    \end{itemize}

    \subsection{Balance}

    \textbf{A BST is balanced if }$\bm{h = O(log n)}$

    \bigskip
    
    \noindent\begin{tabular}{ll}
        \textbf{How to get a Balanced Tree:}\\
        1. Define good property of tree & [AUGMENT]\\
        2. Show that if property holds, tree is balanced. & [DEFINE BALANCE CONDITION]\\
        3. Every insertion/deletion, make sure good property still holds: & \textbf{[INVARIANT]}\\
        -If not, fix it & [MAINTAIN BALANCE]\\
    \end{tabular}

    \pagebreak

    \subsection{AVL Trees}

    \begin{tabular}{l}
        \tabitem Every node, store height $h = max(left.height, right.height) + 1$ \\
        \tabitem On insert \& delete, update height\\
        \tabitem node v is height-balanced if $\bm{|v.left.height - v.right.height| \leq 1}$
        \tabitem Maintains balance using Tree-Rotations
        \tabitem Max height $\bm{h < 2logn}$, $\bm{n > 2^{h/2}}$
    \end{tabular}

    \subsubsection{Rotations}

    \begin{tabular}{l}
        \tabitem A is LEFT-heavy if left.height $>$ right.height\\
        \tabitem A is RIGHT-heavy if right.height $>$ left.height.\\
    \end{tabular}

    \bigskip

    \noindent\begin{tabular}{ll}
        \toprule
        \textbf{Assuming node v is Left-Heavy}\\
        \hline
        \hline
        \tabitem v.left is balanced: & \hyperref[rightrot]{right-rotate(v)}\\
        \hline
        \tabitem v.left is left-heavy: & \hyperref[rightrot2]{right-rotate(v)}\\
        \hline
        \tabitem v.left is right-heavy: & 1. \hyperref[case31]{left-rotate(v.left)}\\
        & 2. \hyperref[case32]{right-rotate(v)}\\
        \bottomrule
        If v is \textbf{Right-Heavy: } & \textbf{Symmetric 3 cases}\\
        \bottomrule
    \end{tabular}

    \bigskip

    Size of tree doesn't matter, $\therefore O(1)$ time.
    
    \subsubsection{Insertion}

    \begin{tabular}{l}
        1. Insert tree in BST \\
        2. Walk up tree:\\
        \tabitem At every step, check for balance:\\
        \tabitem If out-of-balance, use rotations to rebalance\\    
        Only need \textbf{2 Rotations} (Since in all cases, only need to reduce height of sub-tree by 1)\\  
    \end{tabular}
    

    \subsubsection{Deletion}

    \begin{tabular}{l}
        0a. If v has no child, just delete \\
        0b. If v has 1 child, connect child to parent\\
        1. If v has 2 children, swap it with its successor.\\
        2. Delete node v from binary tree (and reconnect children)\\
        \tabitem Since successor has at most 1 (right) child, will only have to reconnect 1 node\\
        3. For every ancestor of the deleted node:\\
        \tabitem Check if it is height-balanced\\
        \tabitem If not, perform a rotation\\
        \tabitem Continue to the root\\
        (\textbf{Deletion may take up to O(logn) rotations})\\        
    \end{tabular}


    \pagebreak
    
    \subsubsection{Graphical Interpretation}

    \begin{figure}[htbp]
        \begin{center}
        \includegraphics[width=5in,keepaspectratio]{Case 1 Right Rotate.JPG}
        \caption{v.left balanced: right-rotate(v)}
        \label{rightrot}
        \end{center}
    \end{figure}

    \begin{figure}[htbp]
        \begin{center}
        \includegraphics[width=5in,keepaspectratio]{case 2 right rotate.JPG}
        \caption{v.left left-heavy: right-rotate(v)}
        \label{rightrot2}
        \end{center}
    \end{figure}

    \begin{figure}[htbp]
        \begin{center}
        \includegraphics[width=5in,keepaspectratio]{Case 3.1.JPG}
        \caption{v.left right-heavy: First left-rotate(v.left)}
        \label{case31}
        \end{center}
    \end{figure}

    \begin{figure}[htbp]
        \begin{center}
        \includegraphics[width=5in,keepaspectratio]{Case 3.2.JPG}
        \caption{v.left right-heavy: then right-rotate(v)}
        \label{case32}
        \end{center}
    \end{figure}


    \pagebreak

    \section{Other (Augmented) Trees}
    \subsection{Tries}
    Store each letter of a String as a node, using a special flag to represent the end of a word.

    \begin{tabular}{l}
        Cost to search a string of length L: $\bm{O(L)}$\\
        Trie tends to be faster compared to normal BST with strings\\
        \tabitem Does not depend on size of total text\\
        \tabitem Does not depend on number of strings (Esp if string not in trie)\\
        Trie uses more space (in terms of more nodes)
    \end{tabular}
    
    \subsection{Order Statistics}

    \begin{tabular}{l}
        \tabitem To know the order of the node (ie rank of the key in the data structure)\\
        \tabitem Store \textbf{size of sub-tree in every node}\\
        \tabitem select(k): finds node with rank k\\
        \tabitem rank(v): Computes rank at node v\\
        \tabitem During insertion, maintain weight during \hyperref[orderstats]{rotation}
    \end{tabular}

    \begin{verbatim}
        select(k)
            rank = left.weight + 1;
            if (k == rank) then
                return v;
            else if (k < rank) then
                return left.select(k);
            else if (k > rank) then
                return right.select(k minus rank);
    \end{verbatim}

    \begin{verbatim}
        rank(node)
            rank = node.left.weight + 1;
            while (node != null) do
                if node is left child then
                    do nothing
                else if node is right child then
                    rank += node.parent.left.weight + 1;
                node = node.parent;
            return rank;
    \end{verbatim}

    \begin{figure}[htbp]
        \begin{center}
        \includegraphics[width=3in,keepaspectratio]{orderstats.JPG}
        \caption{Update weights during insertion}
        \label{orderstats}
        \end{center}
    \end{figure}

    \pagebreak

    \subsection{Interval Trees}

    \begin{tabular}{l}
        \textbf{Find an interval containing a value}\\
        \tabitem Each node is an interval, sorted by \textbf{left endpoint}\\
        \tabitem Each node contains the \textbf{maximum endpoint in subtree}\\
        \tabitem Running time of search simply $O(logn)$\\
    \end{tabular}

    \begin{verbatim}
        //Find interval containing x
        interval-search(x)
            c = root;
            while (c != null and x is not in c.interval) do
                if (c.left == null) then
                    c = c.right;
                else if (x > c.left.max) then
                    c = c.right;
                else c = c.left;
            return c.interval;
    \end{verbatim}

    \bigskip

    \begin{tabular}{l}
        Search find an overlapping interval, if it exists.\\
        \tabitem If search goes right: No overlap in left-subtree\\
        $\therefore$ \textbf{key is in right subtree or it is not in tree}\\
        \tabitem If search goes left and no overlap, then key $<$ every interval in right sub-tree.\\
        $\therefore$ \textbf{Either finds key in left subtree or it is not in the tree}\\
    \end{tabular}

    \pagebreak 

    \subsection{Range Trees/Orthogonal Range Searching}

    \begin{tabular}{l}
        \textbf{Find everyone between a certain range}\\
        \tabitem Stores all points in the \textbf{leaves} (Internal nodes store copies)\\
        \tabitem Internal node v stores \textbf{max(v.left)}\\
        \tabitem First find the 'split node': Is node between specified range?\\
        $\therefore$ Do both Left and Right traversal at split node to get all nodes within range\\
    \end{tabular}

    \begin{verbatim}
        FindSplit(low, high)
            v = root;
            done = false;
            while !done {
                if (high <= v.key) then v=v.left;
                else if (low > v.key) then v=v.right;
                else (done = true);
            }
            return v;
    \end{verbatim}

    \begin{verbatim}
        RightTraversal(v, low, high)
            if (v.key <= high) {                //Still within range
                all-leaf-traversal(v.left);
                RightTraversal(v.right, low, high);
            } else {                           //Left max larger than range, just go left
                RightTraversal(v.left, low, high);
            }
    \end{verbatim}

    \begin{verbatim}
        LeftTraversal(v, low, high)
            if (low <= v.key) {                 //Still within range
                all-leaf-traversal(v.right);
                LeftTraversal(v.left, low, high);
            } else {                            //Left max smaller than range, just go right
                LeftTraversal(v.right, low, high);
            }
    \end{verbatim}

    \bigskip

    \begin{tabular}{l}
        \tabitem Finding split node: $O(logn)$\\
        \tabitem Traversals recurse at most $O(logn)$ times, \\outputting all (all-leaf-traversal())
        is $\bm{O(k)}$, where k is number of items found.\\
        \tabitem $\therefore$ \textbf{Query time complexity } $\bm{ = O(logn + k)}$\\
        \tabitem Preprocessing (buildtree) time complexity: $O(nlogn)$\\
        (Split into left and right, take highest value of left and put as key\\
        If numofelements==1, then set as leaf)\\
        \tabitem Space Complexity: $O(n)$\\
        \tabitem \emph{If just want to know the count: keep count of num of nodes in each sub-tree,} \\
        \emph{and retreive that instead of all-leaf-traversal.}\\
    \end{tabular}

    \bigskip

    \noindent Related: kd-trees (k-dimension)



    \pagebreak

    \section{Hashing}
    
    Standard symbol table supports:

    \begin{tabular}{l}
        \tabitem void insert(key, value)\\
        \tabitem value search(key)\\
        \tabitem void delete(key)\\
        \tabitem bool contains(key)\\
        \tabitem int size()\\\\
    \end{tabular}


    Costs of \textbf{Search} and \textbf{Insert/Delete}, and other functions required: See specifications

    \begin{tabular}{l}
        \tabitem AVL Tree: $O(logn)$ each\\
        \tabitem Symbol Table: $\bm{O(1)}$ each, but extra functionality, eg Sorting ($O(nlogn)$ vs $O(n^{2}$)\\
        \tabitem Symbol Table also no prede/successor queries
        \tabitem \textbf{Since Symbol Tables are not comparison-based}\\
    \end{tabular}

    \subsection{Hash Functions \& Collisions}

    Direct Access Tables take too much space (Number of possible keys very large)\\

    \noindent\textbf{Map keys to buckets using Hash Functions}\\

    \noindent\emph{Assume m buckets, n entries, and h is the hash function,}\\
    
    \begin{tabular}{l}
        \tabitem 2 distinct keys \textbf{collide} if: $h(k_{1}) = h(k_{2}))$\\
        \tabitem Collisions \textbf{unavoidable} by Pigeonhole Principle (Table Size $<$ Universe Size)\\
    \end{tabular}


    \pagebreak

    \subsection{Collision Handling: Chaining}

    

    Put both items in same bucket, using linked List of items.
    \newline

    \begin{tabular}{ll}
        \toprule
        \textbf{Total Space:} & $O(m + n)$\\
        \midrule
        \textbf{Insertion:} & Find hash value, add to head of linked list\\
        &$\therefore O(1 + cost(h))$\\
        \midrule
        \textbf{Search:} & Find hash value, search through linked list\\
        &Worst case all values go to same bucket (emphasizing importance of good hash function)\\
        &$\therefore O(n + cost(h))$\\
        \bottomrule
    \end{tabular}

    \subsubsection{Simple Uniform Hashing Assumption}

    Assume "random" mapping:

    \begin{tabular}{l}
        \tabitem Every key is equally likely to map to every bucket\\
        \tabitem Keys mapped independently\\
        \tabitem $\therefore$ \textbf{As long as enough buckets, won't get too many keys in one bucket}\\\\
    \end{tabular}

    \begin{tabular}{l}
        If $X(i, j) = 1$ if item i is put in bucket j, and $0$ otherwise,\\
        \tabitem $P(X(i, j) == 1) = 1/m$\\
        \tabitem $E(X(i, j)) = 1/m$\\
    \end{tabular}

    \begin{tabular}{ll}
        \tabitem Thus, expected number of items per bucket &= $E(\Sigma_{i}X(i, b))$\\
        &= $\Sigma_{i}E(X(i, b))$\\
        &= $\Sigma_{i}1/m$\\
        &= $n/m$\\
    \end{tabular}

    \begin{tabular}{l}
        \tabitem $\therefore$ \textbf{load(hashtable)} = average number of items per bucket $\bm{= n/m}$\\\\
    \end{tabular}

    Therefore, for a Hashtable with chaining under SUHA assumption:\\

    \begin{tabular}{ll}
        \toprule
        \textbf{Search time:} & $1+ n/m$ (Hash function + linked list traversal)\\
        \tabitem Expected & $O(1)$  (Assuming $m = \Omega(n)$ buckets, eg $m = 2n$)\\
        \tabitem Worst-case & $O(n)$\\
        \midrule
        \textbf{Worst-Case Insertion:} & $O(1)$ if allow duplicates, preventing duplicate requires searching\\
        \midrule
        \textbf{Expected max linked-list length/cost} & $O(logn)$ or $\Theta(logn/loglogn)$\\
        \bottomrule
    \end{tabular}

    \pagebreak

    \subsection{Collision Handling: Open-Addressing}

    
    \begin{tabular}{l}
        \tabitem All data directly stored in the table, one item per slot.\\
        \tabitem On collision, \textbf{probe sequence of buckets until empty one found}\\
        \tabitem When $m == n$, \textbf{table is full, cannot insert any more items}; cannot search efficiently\\
        \tabitem Redefined Hash Function: $h(key, i)$, where i = number of collisions\\
        \tabitem \textbf{Linear Probing:} Keep checking the next bucket, $h(k, 1) + ($i $mod$ m$)$\\
    \end{tabular}


    \begin{verbatim}
        hash-insert(key, data)
        int i = 1;
        while (i <= m):                         // Try every bucket
            int bucket = h(key, i);
            if (T[bucket] == null):             // Found an empty bucket
                T[bucket] = {key, data};        // Insert key/data
                return success;                 // Return
            i++;
        throw new TableFullException();         // bucket full
    \end{verbatim}

    \begin{verbatim}
        hash-search(key)
            int i = 1;
            while (i <= m):
                int bucket = h(key, i);
                if (T[bucket] == null) return key-not-found;        // Empty bucket!
                if (T[bucket].key == key) return T[bucket].data;    // Full bucket
                i++;
            return key-not-found;                                   // Exhausted entire table.
    \end{verbatim}

    \begin{tabular}{l}
        \textbf{delete(key):} Find key to delete, \textbf{set bucket to DELETED (A tombstone value)}\\
        \tabitem Cannot set as NULL, since search may then fail to find a key after that bucket.\\
        \tabitem When insert(key) comes to DELETED, \textbf{overwrite deleted cell}.\\\\
    \end{tabular}

    \begin{tabular}{l}
        
    \end{tabular}

    \subsubsection{Properties of good Hash Functions}

    
    \begin{tabular}{l}
        \textbf{1.} $\bm{h(key, i)}$ \textbf{enumerates all possible buckets}\\
        \tabitem $\forall$ bucket $j, \exists$ i $: h(key, i) = j$\\
        \tabitem The hash function is permutation of ${1...m}$\\
        \tabitem If not, may return table-full when still have space left\\\\
    \end{tabular}
    
    \noindent\begin{tabular}{l}
        \textbf{2. Uniform Hashing Assumption}\\
        \tabitem Every key is equally likely to be mapped to every \textbf{permutation of buckets}, independent of every other key.\\
        \tabitem Linear Probing does NOT fulfill this criteria: \textbf{Clustering} can reach $\Theta(logn)$, ruins constant time performance\\
    \end{tabular}

    \emph{In practice though, linear probing is desirable due to caching}

    \noindent\begin{tabular}{l}
        \tabitem\textbf{Achieved through double hashing}\\
        \tabitem Using 2 hash functions $g(k), f(k)$, $\bm{h(k, i) = [f(k) + ig(k)]}$ $\bm{mod}$ \textbf{m} for some large m\\
    \end{tabular}

    \emph{Specifically, if g(k) is relatively prime to m, then h(k, i) hits all buckets}

    \pagebreak

    \subsubsection{Performance of Open Addressing}
    \noindent\begin{tabular}{l}
        \textbf{Expected Cost = First Probe + P(collision on first probe) * Expected Cost of remaining probes}\\
        \tabitem $= 1 + (n/m)(...)$\\
        \tabitem $= 1 + (n/m)(1 + [n-1/m-1][...])$\\
        \tabitem $\leq 1 + \alpha(1 + \alpha(...))$\\
        \tabitem $\leq 1 + \alpha + \alpha^{2} + \alpha^{3} + ...$\\
        \tabitem $\leq \frac{1}{1 - \alpha}$\\\\
    \end{tabular}

    \noindent\begin{tabular}{l}
        \textbf{Advantages}\\
        \tabitem Saves space\\
        \tabitem Rarely Allocate Memory\\
        \tabitem Better Cache performance\\
        \textbf{Disadvantages}\\
        \tabitem More sensitive to choice of hash functions\\
        \tabitem More sensitive to load (as $\alpha \rightarrow 1$)
    \end{tabular}

    \subsection{Resizing}

    \noindent\begin{tabular}{l}
        Assume\\
        \tabitem Hashing with Chaining\\
        \tabitem SUHA\\
        \textbf{Expected Search Time:} $O(1 + n/m)$\\
        \textbf{Optimal Size:} $m = O(n)$\\\\
    \end{tabular}

    If m too big ($> 10n$), too much wasted space; if m too small ($< 2n$), too many collisions\\
    
    \noindent\begin{tabular}{l}
        \textbf{To expand hashtable:}, let $m_{1} and m_{2}$ be old and new hashtable size\\
        \tabitem Scan old hash table: $O(m_{1})$, Initialise new table: $O(m_{2})$\\
        \tabitem Insert each element in new hashtable: $O(1) * n$\\
        \tabitem \textbf{Total: } $\bm{O(m_{1} + m_{2} + n)}$\\
        \tabitem If double table size, $(n == m), m = 2m$: $O(n)$ time\\\\
    \end{tabular}

    \noindent\begin{tabular}{l}
        \textbf{To shrink hashtable:}, let $m_{1} and m_{2}$ be old and new hashtable size\\
        \tabitem Cannot be same ratio as insert, cos there will be a point where deleting/inserting 1 shrinks/expands the table\\
        If insert doubles the table, then for delete:\\
        \tabitem If $(n < m/4), m = m/2$\\\\
    \end{tabular}

    \noindent\begin{tabular}{l}
        \textbf{Costs of operations:}\\
        \tabitem Inserting k elements costs $O(k)$\\
        \tabitem $\therefore$ Insert operation: \textbf{Amortized }$\bm{O(1)}$\\
        \tabitem Search operation: \textbf{Expected }$\bm{O(1)}$\\\\
    \end{tabular}

    \pagebreak

    \section{Sets}

    insert(Key k), contains(Key k), delete(Key k), intersect(Set<Key> s), union(Set<Key> s)

    \subsection{Implementation using Hashtable}

    Takes more space to keep the entire key (to resolve collisions) in the table.

    \subsection{Fingerprint Hashtable}

    \noindent\begin{tabular}{l}
        Stores bits (0 and 1) instead of the key, 0 if not present, 1 if present. No key stored in the table.\\
        \tabitem \textbf{Collisions possible}\\
        \tabitem Lookup operation: If key is \textbf{in}, will always report true (\textbf{No False Negatives})\\
        \tabitem Due to collisions, even in key not in set, may sometimes report true (\textbf{False Positives})\\
        Thus choosing what to store is important, based on objectives\\
    \end{tabular}

    \subsubsection{Table Size vs P(False Positives)}

    \begin{tabular}{l}
        On a lookup of n elements of table of size m,\\
        \tabitem P(No false positive) = $(1 - 1/m)^{n} \approx (1/e)^{n/m}$\\
        \tabitem P(False positive) = $1 - (1/e)^{n/m}$\\\\
        Assuming we want P(false positive) at most p:\\s
        \tabitem $n/m \leq log(\frac{1}{1-p})$\\
        So we reduced space to 1 bit per slot, but need a bigger table to avoid collisions\\\\
    \end{tabular}
    
    \subsection{Bloom Filter}

    \begin{tabular}{l}
        Fingerprint Hashtable, but 2 hash function to \textbf{store 1 in 2 different slots}.\\
        \tabitem Lookup: Check if both slots are 1\\
        \tabitem Still,\textbf{No False Negatives} and possible \textbf{False Positives}\\
        Requires 2 collisions to be a false positive, but each item take more space.\\\\
        Assuming we want P(false positive) at most p:\\
        \tabitem $n/m \leq \frac{1}{2}log(\frac{1}{1-p^{1/2}})$\\\\
        Deleting elements? Consider a counter instead of 1 bit in each slot:\\
        \tabitem On insert, counter++\\
        \tabitem On delete, counter--\\
        If counter gets too big, no space saving: Thus need to make collisions rare\\\\
        Implementing Set functions:\\
        \tabitem Insert, delete, query: $O(k)$\\
        \tabitem Intersection, Bitwise AND 2 bloom filters: $O(m)$\\
        tabitem Union, Bitwise OR 2 bloom filters: $O(m)$\\
    \end{tabular}

    \pagebreak

    \section{Other Data Structures}

    \subsection{(a, b)-trees and B-trees}

    \begin{tabular}{l}
        \toprule
        \textbf{a, b refer to min and (max + 1) no. of children in node, where }$\bm{2 \leq a \leq (b+1)/2}$\\
        \midrule
        Non-leaf node must have one more child than its number of keys, its \textbf{key range:}\\
        \tabitem Keys in sorted order, $v_{1}, v_{2},...v_{k}$\\
        \tabitem First child has key range $\leq v_{1}$\\
        \tabitem Final child has key range $> v_{k}$\\
        \tabitem All other children $c_{i}$, where $i \in [2, k]$ have key range $(v_{i-1}, v_{i}]$\\
        \midrule
        All leaf nodes must be same depth\\
        \midrule
        \textbf{Insert:} split node if contain b-1 keys (Node too big)\\
        \midrule
        \textbf{Delete:} if deleting make node too small, merge siblings y,z if have total nodes $\leq b-1$, \\
        else share by merging and splitting\\
        \bottomrule\\
    \end{tabular}

    \noindent\textbf{B-trees} are (a, b)-trees such that $a = B$, $b = 2B$

    \begin{figure}[htbp]
        \begin{center}
        \includegraphics[width=5in,keepaspectratio]{Btree.JPG}
        \caption{B-tree, where B = 2}
        \end{center}
    \end{figure}

    \subsection{Skip Lists}

    \subsection{Merkle Trees}

    \pagebreak

    \section{Graphs}

    \begin{tabular}{l}
        Consists of at least 1 node, and unique edges that connect 2 nodes\\
        \textbf{Hypergraph}: Eaah unique edge connect $\geq 2$ nodes\\
        \textbf{Multigraph}: Each node connected by more than 1 edge\\
        Degree of node: Number of adjacent edges\\
        Degree of graph: max(degree of nodes)\\
        Diameter: Max distance between 2 nodes, following shortest path\\
        \textbf{Bipartite graph}: Nodes divided to 2 sets, no edges between same set\\
    \end{tabular}

    \subsection{Adjacency list}

    \begin{tabular}{l}
        Nodes stored in an array, Edges stored as linked list per node\\\\
        \textbf{Memory Usage: }$\bm{O(V + E)}$, since of array V and size of linked lists E\\\\
        Are v and w neighbours? \textbf{Fast query}\\
        Find any neighbour of v: \textbf{Slow query}\\
        Enumerate all neighbours: \textbf{Slow query}\\
    \end{tabular}

    \subsection{Adjacency Matrix}

    \begin{tabular}{l}
        Edges seen as pairs of nodes. For a graph with n nodes, nxn array:\\
        At A[i][j], 1 if i and j are directly connected\\
        $A^{n}$: Length of n paths\\\\
        \textbf{Memory Usage:} $\bm{O(V^{2})}$\\\\
        Are v and w neighbours? \textbf{Slow query}\\
        Find any neighbour of v: \textbf{Fast query}\\
        Enumerate all neighbours: \textbf{Fast query}\\\\
    \end{tabular}

    \noindent\textbf{Generally, if graph is dense, use an adjacency matrix, if not then adjacency list}

    \pagebreak
    
    \section{Graph Traversal}

    Start at vertex s, ends at vertex t, or visit all nodes in the graph. (Assume adjacency list)
    

    \subsection{Breadth-First Search}

    \begin{tabular}{l}
        \tabitem \textbf{Finds shortest paths}\\
        \tabitem Skip already visited nodes, calculate level[i] from level[i-1]\\
    \end{tabular}

    \begin{verbatim}
    //Or can use a Queue to pop the earlier ones first
    BFS(Node[] nodeList) {
        boolean[] visited = new boolean[nodeList.length];
        Arrays.fill(visited, false);

        int[] parent = new int[nodelist.length];
        Arrays.fill(parent, -1);

        // To make sure you visit all components 
        for (int start = 0; start < nodeList.length; start++) {
            if (!visited[start]){
                Bag<Integer> frontier = new Bag<Integer>;
                frontier.add(startId);

                // Main code
                while (!frontier.isEmpty()){
                    Collection<Integer> nextFrontier = new … ;
                    for (Integer v : frontier) {
                        for (Integer w : nodeList[v].nbrList) {
                            if (!visited[w]) {
                                visited[w] = true;
                                parent[w] = v;
                                nextFrontier.add(w);
                            }
                        }
                    }
                    frontier = nextFrontier;
                }
            }
        }
    }
    \end{verbatim}

    \begin{tabular}{l}
        \textbf{Running Time:} $\bm{O(V + E)}$ assuming adjacency list\\
        \tabitem Every vertex v = start once, and added to nextFrontier once \\
        (After visited, never re-added: $O(V)$)\\
        \tabitem Each v.nbrList enumerated once: $O(E)$\\\\
        Shortest path is a tree - Parent pointers store shortest path   
    \end{tabular}

    \pagebreak

    \subsection{Depth-first search}



    \pagebreak

    \section{Data Structures Summary}

    \subsection{Trees}

    \noindent\begin{tabular}{|l|l|l|l|p{5cm}|}
        \toprule
        \textbf{Name} & \textbf{Search} & \textbf{Insert} & \textbf{Delete} & \textbf{Remarks}\\
        \midrule
        \midrule
        \textbf{BST} & $O(height)$ & $O(height)$ & $O(height)$ & $h < 2log(n)$\\
        \midrule
        \textbf{AVL} & $O(logn)$ & $O(logn) +$ 2 rotations & $O(logn)$ + logn rotations & If v is left-heavy,\\
        &&&&- v.left is balanced/left-heavy: right-rotate(v)\\
        &&&&- v.left is right-heavy: left-rotate(v.left), right-rotate(v)\\
        \midrule
        \textbf{Trie} & $O(length)$ & $O(length)$& $O(length)$ &\\
        \midrule
        \textbf{(a,b)-trees} & $O(logn)$ & $O(logn)$ & $O(logn)$ & Insert: split\\
        \textbf{B-trees} &&&& Delete: Merge, or Share (merge + split)\\
        \bottomrule
    \end{tabular}

    \subsection{Augmented Trees}

    Assuming augmented from AVL, search(), insert() and delete() are $O(logn)$\\

    \noindent\begin{tabular}{|l|l|}
        \toprule
        \textbf{Order Statistics} & \textbf{Find order/rank of nodes}\\
        & \tabitem Store size of sub-tree in every node\\
        & \tabitem During insertion, maintain weight during rotation\\
        \midrule
        \textbf{Interval Tree} & Nodes sorted by left endpoint\\
        & Nodes contain max endpoint in tree rooted at node\\
        \midrule 
        \textbf{Orthogonal Range Searching} & \textbf{Find everything within certain range}\\
        (kd-trees) & \tabitem Points stored in leaves\\
        & \tabitem internal node stores max(node.left)\\
        & \tabitem kd-trees: Alterate splitting between dimensions:\\
        & \tabitem Query: $O(\sqrt{n} + k)$, Space: $O(n)$, Build: $O(nlogn)$\\
        \midrule
        \textbf{Range Tree} & \textbf{Build x-tree using only x-coords, x-node contains y-tree (etc)}\\
        & \tabitem Query: $O(log^{2}n + k)$, Space: $O(nlogn)$, Build: $O(nlogn)$\\
        \bottomrule 
    \end{tabular}

    \subsection{Hashing}

    Assuming m is number of buckets, n is number of keys, h is cost of hash function,\\

    \noindent\begin{tabular}{|l|l|l|l|p{6.5cm}|}
        \toprule
        \textbf{Name} & \textbf{Search} & \textbf{Insert} & \textbf{Space} & \textbf{Remarks}\\
        \midrule
        \midrule
        \textbf{Chaining} & $O(h + n/m)$ & $O(h + 1)$ & $O(m+n)$ & \tabitem Simple Uniform Hashing Assumption\\
        & $= O(1)$ (Expected) &&&\tabitem load $= n/m$ \\
        & $= O(n)$ (Worst-case) &&&\\
        \midrule
        \textbf{Open-Addressing} &$O(1)$&$1/(1-load)$& $O(n)$ &\tabitem Uniform Hashing Assumption\\
        &&&&\tabitem Redefine hash function: Linear Probing or otherwise\\
        &&&&\tabitem Double-hashing:\\
        &&&& $h(k, i) = [f(k) + ig(k)]$ $mod$ m\\
        &&&&\tabitem Tombstone value for deleted items\\
        &&&&\tabitem Performance degrades as load $= n/m$ tends to 1\\
        \bottomrule
    \end{tabular}


    

    

\end{document}


